q()
q()
q()
q()
defaults write org.R-project.R force.LANG en_US.UTF-8
q()
install.packages("ggplot")
install.packages("ggplot2")
jupyter notebook
q()
install.packages("sp")
install.packages('gglplot2', dependencies = TRUE)
install.packages('ggplot2', dependencies = TRUE)
warnings()
q()
remove.packages("ggplot2")
install.packages('ggplot2', dependencies = TRUE)
q()
install.packages("sp")
conda install clangxx_osx-64
conda install clangxx_osx-64
q()
install.packages("twitteR")
install.packages(c("devtools", "rjson", "bit64", "httr"))
q()
library(twitteR)
install.packages("twitteR")
q(0
q()
q()
install.packages("twitteR", dependencies = TRUE)
library(twitteR)
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A", "owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m")
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A", "owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m")
install.packages("httpuv")
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A", "owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m")
install.packages("httpuv", dependencies=TRUE)
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A", "owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m")
devtools::install_github("rstudio/httpuv")
install_github("rstudio/httpuv")
getTwitterOAuth(consumer_key, consumer_secret)
getTwitterOAuth(VxJ6qp5XL3VTclBzMBsD1Ez1A, owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m)
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A", "owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m")
remove.packages("httpuv")
install.packages("httpuv")
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A", "owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m")
cred <- OAuthFactory$new(consumerKey='VxJ6qp5XL3VTclBzMBsD1Ez1A',
consumerSecret='owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m',
requestURL='https://api.twitter.com/oauth/request_token',
accessURL='https://api.twitter.com/oauth/access_token',
authURL='https://api.twitter.com/oauth/authorize')
search.string <- "#flu"
no.of.tweets <- 100
tweets <- searchTwitter(search.string, n=no.of.tweets, cainfo="cacert.pem", lang="en")
tweets
search.string <- "#flu"
no.of.tweets <- 100
tweets <- searchTwitter(search.string, n=no.of.tweets,lang="en")
tweets
View(tweets)
View(tweets)
View(tweets)
search.string <- "#flu"
no.of.tweets <- 10000
tweets <- searchTwitter(search.string, n=no.of.tweets, geocode="40.7128,74.0060,150mi", lang="en")
tweets
View(tweets)
View(tweets)
View(tweets)
tweets
search.string <- "#flu"
no.of.tweets <- 10000
tweets <- searchTwitter(search.string, n=no.of.tweets, geocode="40.7128,74.0060,150mi", lang="en")
tweets
View(tweets)
View(tweets)
View(tweets)
View(tweets)
search.string <- "#flu"
no.of.tweets <- 100
tweets <- searchTwitter(search.string, n=no.of.tweets, lang="en")
tweets
View(tweets)
View(tweets)
View(tweets)
View(tweets)
View(tweets)
View(tweets)
search.string <- "#flu"
no.of.tweets <- 100
tweets <- searchTwitter(search.string, n=no.of.tweets, lang="en")
tweets
View(tweets)
View(tweets)
rm(list =ls())
## loading libraries
library(twitteR)
library(ggplot2)
library(ggmap)
## Setup oauth
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A", "owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m", "340449785-0AWt3nkBVvLlX7hbUFLl0fEqIKs47qUU7V5UnFWH", "qnaD0Pyp9jUXfwVb82RlSKikuvVi2MAWxp1J0mD1Fle4d")
## Searching for tweets ##
search.string <- "flu"
no.of.tweets <- 15000
tweets <- searchTwitter(search.string, n=no.of.tweets, lang="en")
rm(list =ls())
## loading libraries
library(twitteR)
library(ggplot2)
library(ggmap)
## Setup oauth
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A", "owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m", "340449785-0AWt3nkBVvLlX7hbUFLl0fEqIKs47qUU7V5UnFWH", "qnaD0Pyp9jUXfwVb82RlSKikuvVi2MAWxp1J0mD1Fle4d")
## Searching for tweets ##
search.string <- "flu"
no.of.tweets <- 5000
tweets <- searchTwitter(search.string, n=no.of.tweets, lang="en")
search.string <- "flu"
no.of.tweets <- 5000
tweets <- searchTwitter(search.string, n=no.of.tweets, lang="en")
Name=Sys.Date()+Sys.time()
Name=paste(Sys.Date(),Sys.time(),sep = ";")
Name=Sys.time()
Name=paste("Data Collected on",Sys.time(),sep = ";")
Name=paste("Data Collected on ",Sys.time(),)
Name=paste("Data Collected on ",Sys.time())
rm(list =ls())
## loading libraries
library(twitteR)
library(ggplot2)
library(ggmap)
## Setup oauth
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A", "owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m", "340449785-0AWt3nkBVvLlX7hbUFLl0fEqIKs47qUU7V5UnFWH", "qnaD0Pyp9jUXfwVb82RlSKikuvVi2MAWxp1J0mD1Fle4d")
## Searching for tweets ##
search.string <- "flu"
no.of.tweets <- 100
tweets <- searchTwitter(search.string, n=no.of.tweets, lang="en")
## Conversion of searched tweets to Data frame
tweets <- twListToDF(tweets)
## Saving collected data to a csv file
Name=paste("Data Collected on ",Sys.time())
write.csv(tweets, file = Name)
install.packages("choroplethr")
data$ACTIVITY.LEVEL=as.numeric(data$ACTIVITY.LEVEL)
library(plotly)
?Sys.timezone
a <- ?Sys.timezone
Sys.getenv("TZ")
Sys.setenv("TZ")
as.POSIXct(t, tz=getOption("tz"))
install.packages("plotly")
Sys.setenv
Sys.setenv(TZ = "America/New_York")
install.packages("plotly")
library(reshape2)
library(rjson)
a<-read.table("testdata")
setwd("~/Documents/GitHub/Data-Integration-Big-Data-Analysis-And-Visualization/Part2/code/d3jsVisualization")
a<-read.table("testdata")
b<-melt(a, id='V1', variable_name='V2')
c <- toJSON(setNames(a, rownames(df)))
save(c, file="export.JSON")
c
View(b)
View(a)
colnames(a) <- c("text","size")
c <- toJSON(setNames(a, rownames(df)))
c
setwd("~/Documents/GitHub/Data-Integration-Big-Data-Analysis-And-Visualization/Part2/code/dataCollection")
## Saving the article extracted
setwd("../../Data")
alpha <- read.table("articlesTotal.txt")
alpha <- read("articlesTotal.txt")
library(readtext)
install.packages("readtext")
library(readtext)
alpha <- system.file("articlesTotal.txt")
alpha
library(reshape2)
library(rjson)
a<-read.table("testdata")
colnames(a) <- c("text","size")
c <- toJSON(a, dataframe="rows")
library(reshape2)
library(rjson)
a<-read.table("testdata")
setwd("~/Documents/GitHub/Data-Integration-Big-Data-Analysis-And-Visualization/Part2/code/d3jsVisualization")
library(reshape2)
library(rjson)
a<-read.table("testdata")
colnames(a) <- c("text","size")
c <- toJSON(a, dataframe="rows")
c <- toJSON(a, dataframe="rows")
View(a)
c <- toJSON(a, dataframe="text")
library(jsonlite)
a<-read.table("testdata")
colnames(a) <- c("text","size")
c <- toJSON(a, dataframe="rows")
save(c, file="export.JSON")
View(a)
install.packages("hashmap")
library(hashmap)
c<- clone(a)
c<- hashmap(a)
View(a)
c<- hashmap(a$text,a$size)
View(c)
c
clone(c)
View(a)
View(a)
write.csv(a,"word2.csv",col.names = F,row.names = F)
write.csv(a,"word2.txt",col.names = F,row.names = F)
library(reshape2)
library(jsonlite)
library(hashmap)
a<-read.table("testdata")
colnames(a) <- c("text","size")
c <- toJSON(as.list(setNames(a,rownames(df))),auto_unbox=TRUE)
c <- toJSON(as.list(setNames(df$Values,rownames(df))),auto_unbox=TRUE, dataframe = "rows")
c <- toJSON(a, dataframe = "rows")
c <- toJSON(a, dataframe = "rows")
c
a <- a[1:10,]
c <- toJSON(a, dataframe = "rows")
c
a<-read.table("testdata")
colnames(a) <- c("text","size")
a <- a[3:14,]
c <- toJSON(a, dataframe = "rows")
save(c, file="export.JSON")
a<-read.table("testdata")
colnames(a) <- c("text","size")
a <- a[3:14,]
c <- toJSON(a, dataframe = "rows")
c <- toJSON(a, dataframe = "rows", simplifyDataFrame = simplifyVector)
c <- toJSON(a, dataframe = "rows", simplifyDataFrame = simplifyVector, factor = c("string", "integer"),)
c <- toJSON(a, dataframe = "rows", simplifyDataFrame = simplifyVector, factor = c("integer", "integer"),)
c <- toJSON(a, dataframe = "rows", simplifyDataFrame = simplifyVector, factor = c("string", "string"),)
c <- toJSON(a, dataframe = "rows", simplifyDataFrame = simplifyVector, auto_unbox = TRUE)
a <- idw.output[,1:3]
datasetres <- a[,1:3]
a<-read.table("testdata")
datasetres <- a[,1:3]
a<-read.table("testdata")
colnames(a) <- c("text","size")
a <- a[3:14,]
c <- toJSON(a, dataframe = "rows", simplifyDataFrame = simplifyVector, auto_unbox = TRUE)
save(c, file="export.JSON")
c
a<-read.csv("testdata.csv")
colnames(a) <- c("text","size")
a <- a[3:14,]
c <- toJSON(a, dataframe = "rows", simplifyDataFrame = simplifyVector, auto_unbox = TRUE)
save(c, file="export.JSON")
c
a<-read.csv("testdata.csv")
colnames(a) <- c("text","size")
View(a)
a<-read.txt("testdata.csv")
a<-read.table("testdata.csv")
View(a)
colnames(a) <- c("text","size")
a <- a[3:14,]
c <- toJSON(a, dataframe = "rows", simplifyDataFrame = simplifyVector, auto_unbox = TRUE)
save(c, file="export.JSON")
c
a<-read.table("testdata.csv")
colnames(a) <- c("text","size")
##a <- a[3:14,]
c <- toJSON(a, dataframe = "rows", simplifyDataFrame = simplifyVector, auto_unbox = TRUE)
save(c, file="export.JSON")
c
a<-read.table("testdata.csv")
colnames(a) <- c("text","size")
##a <- a[3:14,]
words <- toJSON(a, dataframe = "rows", simplifyDataFrame = simplifyVector, auto_unbox = TRUE)
save(words, file="export.JSON")
save(words, file="export.JSON")
a<-read.table("tweets.csv")
View(a)
write.csv("tweets.csv")
a<-read.table("tweets.csv")
write.csv(a,"tweets.csv", row.names = F)
write.csv(a,"tweets.txt", row.names = F)
data <- read.table("tweetdata_top50.csv")
setwd("~/Documents/GitHub/Data-Integration-Big-Data-Analysis-And-Visualization/Part2/output/Top_50_Coocurringwords")
data <- read.table("tweetdata_top50.csv")
data$V1
View(data)
