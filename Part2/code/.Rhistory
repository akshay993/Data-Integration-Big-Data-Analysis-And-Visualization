content <-data.frame(Data)
content$Content <- str_replace_all(content$Content, "[^[:alnum:]]", " ")  ## removes special characters
wordcount <- length(unlist(strsplit(toString(content$Content[1])," "))) ## gets a count of the number of words in the article
content <- cbind(content,wordcount)
}
if (i>1){
try(Data <- ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
temp <- data.frame(Data)
temp$Content <- str_replace_all(temp$Content, "[^[:alnum:]]", " ") ##removes special characters
wordcount <- length(unlist(strsplit(toString(temp$Content[1])," "))) ## gets a count of the number of words in the article
temp <- cbind(temp,wordcount)
content <- rbind(content,temp)
}
print(i)
}
library('RCurl')
library('XML')
library(Rcrawler)
for (i in c(1:nrow(Articles))){
if (i==1){
try(Data<-ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
content <-data.frame(Data)
content$Content <- str_replace_all(content$Content, "[^[:alnum:]]", " ")  ## removes special characters
wordcount <- length(unlist(strsplit(toString(content$Content[1])," "))) ## gets a count of the number of words in the article
content <- cbind(content,wordcount)
}
if (i>1){
try(Data <- ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
temp <- data.frame(Data)
temp$Content <- str_replace_all(temp$Content, "[^[:alnum:]]", " ") ##removes special characters
wordcount <- length(unlist(strsplit(toString(temp$Content[1])," "))) ## gets a count of the number of words in the article
temp <- cbind(temp,wordcount)
content <- rbind(content,temp)
}
print(i)
}
library(stringr)
for (i in c(1:nrow(Articles))){
if (i==1){
try(Data<-ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
content <-data.frame(Data)
content$Content <- str_replace_all(content$Content, "[^[:alnum:]]", " ")  ## removes special characters
wordcount <- length(unlist(strsplit(toString(content$Content[1])," "))) ## gets a count of the number of words in the article
content <- cbind(content,wordcount)
}
if (i>1){
try(Data <- ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
temp <- data.frame(Data)
temp$Content <- str_replace_all(temp$Content, "[^[:alnum:]]", " ") ##removes special characters
wordcount <- length(unlist(strsplit(toString(temp$Content[1])," "))) ## gets a count of the number of words in the article
temp <- cbind(temp,wordcount)
content <- rbind(content,temp)
}
print(i)
}
content = content[!duplicated(content$wordcount),]
View(Articles)
View(content)
setwd("../code")
"ab"&&"dd"
"ab"&"dd"
paste("dd","dd")
setwd("../Data/Day 1/NYTimes_Articles")
setwd("~/Documents/GitHub/Data-Integration-Big-Data-Analysis-And-Visualization/Part2/code")
for (i in c(1:nrow(content)))
{
setwd("../Data/Day 1/NYTimes_Articles")
file_name=paste("March 20th - Article",i)
write.table(content[1],file_name)
}
for (i in c(1:nrow(content)))
{
setwd("../Data/Day_1/NYTimes_Articles")
file_name=paste("March 20th - Article",i)
write.table(content[1],file_name)
}
setwd("../Data/Day_1/NYTimes_Articles")
setwd("../Data/Day_1/NYTimes_Articles")
setwd("~/Documents/GitHub/Data-Integration-Big-Data-Analysis-And-Visualization/Part2/code")
setwd("../Data/Day_1/NYTimes_Articles")
for (i in c(1:nrow(content)))
{
file_name=paste("March 20th - Article",i)
write.table(content[1],file_name)
}
View(Articles)
View(content)
sum(content$wordcount)
content[22]
content$Content[22]
setwd("../Data/Day_1/NYTimes_Articles")
for (i in c(1:nrow(content)))
{
file_name=paste("March 20th - Article",i)
write.table(content$Content[1],file_name,  sep="\t", col.names = F, row.names = F)
}
View(content)
length(unlist(strsplit(toString(temp$Content[1])," "))) <- content$Content[22]
length(unlist(strsplit(toString(content$Content[22])," ")))
for (i in c(1:nrow(Articles))){
if (i==1){
try(Data<-ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
content <-data.frame(Data)
content$Content <- str_replace_all(content$Content, "[^[:alnum:]]", " ")  ## removes special characters
wordcount <- length(unlist(strsplit(toString(content$Content[i])," "))) ## gets a count of the number of words in the article
content <- cbind(content,wordcount)
}
if (i>1){
try(Data <- ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
temp <- data.frame(Data)
temp$Content <- str_replace_all(temp$Content, "[^[:alnum:]]", " ") ##removes special characters
wordcount <- length(unlist(strsplit(toString(temp$Content[i])," "))) ## gets a count of the number of words in the article
temp <- cbind(temp,wordcount)
content <- rbind(content,temp)
}
print(i)
}
View(content)
for (i in c(1:nrow(Articles))){
if (i==1){
try(Data<-ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
content <-data.frame(Data)
content$Content <- str_replace_all(content$Content, "[^[:alnum:]]", " ")  ## removes special characters
wordcount <- length(unlist(strsplit(toString(content$Content[i])," "))) ## gets a count of the number of words in the article
content <- cbind(content,wordcount)
}
if (i>1){
try(Data <- ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
temp <- data.frame(Data)
temp$Content <- str_replace_all(temp$Content, "[^[:alnum:]]", " ") ##removes special characters
wordcount <- length(unlist(strsplit(toString(temp$Content)," "))) ## gets a count of the number of words in the article
temp <- cbind(temp,wordcount)
content <- rbind(content,temp)
}
print(i)
}
View(content)
View(content)
for (i in c(1:nrow(Articles))){
if (i==1){
try(Data<-ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
content <-data.frame(Data)
content$Content <- str_replace_all(content$Content, "[^[:alnum:]]", " ")  ## removes special characters
wordcount <- length(unlist(strsplit(toString(content$Content[i])," "))) ## gets a count of the number of words in the article
content <- cbind(content,wordcount)
}
if (i>1){
try(Data <- ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
temp <- data.frame(Data)
temp$Content <- str_replace_all(temp$Content, "[^[:alnum:]]", " ") ##removes special characters
wordcount <- length(unlist(strsplit(toString(temp$Content)," "))) ## gets a count of the number of words in the article
if (i==2)
{
asadf <- 1
}
temp <- cbind(temp,wordcount)
content <- rbind(content,temp)
}
print(i)
}
for (i in c(1:nrow(Articles))){
if (i==1){
try(Data<-ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
content <-data.frame(Data)
content$Content <- str_replace_all(content$Content, "[^[:alnum:]]", " ")  ## removes special characters
wordcount <- length(unlist(strsplit(toString(content$Content[i])," "))) ## gets a count of the number of words in the article
content <- cbind(content,wordcount)
}
if (i>1){
try(Data <- ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
temp <- data.frame(Data)
temp$Content <- str_replace_all(temp$Content, "[^[:alnum:]]", " ") ##removes special characters
wordcount <- length(unlist(strsplit(toString(temp$Content)," "))) ## gets a count of the number of words in the article
if (i==22)
{
asadf <- 1
}
temp <- cbind(temp,wordcount)
content <- rbind(content,temp)
}
print(i)
}
# Eliminating duplicates caused by error handling
#
debugSource('~/Documents/GitHub/Data-Integration-Big-Data-Analysis-And-Visualization/Part2/code/articleExtraction.r')
debugSource('~/Documents/GitHub/Data-Integration-Big-Data-Analysis-And-Visualization/Part2/code/articleExtraction.r')
debugSource('~/Documents/GitHub/Data-Integration-Big-Data-Analysis-And-Visualization/Part2/code/articleExtraction.r', echo=TRUE)
i=22
try(Data <- ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
temp <- data.frame(Data)
temp$Content <- str_replace_all(temp$Content, "[^[:alnum:]]", " ") ##removes special characters
wordcount <- length(unlist(strsplit(toString(temp$Content)," "))) ## gets a count of the number of words in the article
temp$Content
temp <- cbind(temp,wordcount)
View(temp)
temp1<-temp
for (i in c(1:nrow(Articles))){
if (i==1){
try(Data<-ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
content <-data.frame(Data)
content$Content <- str_replace_all(content$Content, "[^[:alnum:]]", " ")  ## removes special characters
wordcount <- length(unlist(strsplit(toString(content$Content[i])," "))) ## gets a count of the number of words in the article
content <- cbind(content,wordcount)
}
if (i>1){
try(Data <- ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
temp <- data.frame(Data)
temp$Content <- str_replace_all(temp$Content, "[^[:alnum:]]", " ") ##removes special characters
wordcount <- length(unlist(strsplit(toString(temp$Content)," "))) ## gets a count of the number of words in the article
temp <- cbind(temp,wordcount)
content <- rbind(content,temp)
}
print(i)
}
View(content)
setwd("../Data/Day_1/NYTimes_Articles")
for (i in c(1:nrow(content)))
{
file_name=paste("March 20th - Article",i)
write.table(content$Content[1],file_name,  sep="\t", col.names = F, row.names = F)
}
sum(content$wordcount)
Start_date <- 2018-03-20
library(twitteR)
library(ggplot2)
library(ggmap)
library(data.table)
library(stringr)
Start_date <- 2018-03-20
End_date <- 2018-03-21
## Setup oauth
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A",
"owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m",
"340449785-0AWt3nkBVvLlX7hbUFLl0fEqIKs47qUU7V5UnFWH",
"qnaD0Pyp9jUXfwVb82RlSKikuvVi2MAWxp1J0mD1Fle4d")
############## Collection of Tweets ###################
## Searching for tweets ##
search.string <- "#deletefacebook"
no.of.tweets <- 2500
tweets <- searchTwitter(search.string, n=no.of.tweets, lang="en", since= Start_date , until = End_date)
##rm(list =ls())
#### Code to collect tweets##
## All tweets collected are filter and appended to csv files in data folder automatically
library(twitteR)
library(ggplot2)
library(ggmap)
library(data.table)
library(stringr)
Start_date <- "2018-03-20"
End_date <- "2018-03-21"
## Setup oauth
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A",
"owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m",
"340449785-0AWt3nkBVvLlX7hbUFLl0fEqIKs47qUU7V5UnFWH",
"qnaD0Pyp9jUXfwVb82RlSKikuvVi2MAWxp1J0mD1Fle4d")
############## Collection of Tweets ###################
## Searching for tweets ##
search.string <- "#deletefacebook"
no.of.tweets <- 2500
tweets <- searchTwitter(search.string, n=no.of.tweets, lang="en", since= Start_date , until = End_date)
##rm(list =ls())
#### Code to collect tweets##
## All tweets collected are filter and appended to csv files in data folder automatically
library(twitteR)
library(ggplot2)
library(ggmap)
library(data.table)
library(stringr)
Start_date <- "2018-03-20"
End_date <- "2018-03-21"
## Setup oauth
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A",
"owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m",
"340449785-0AWt3nkBVvLlX7hbUFLl0fEqIKs47qUU7V5UnFWH",
"qnaD0Pyp9jUXfwVb82RlSKikuvVi2MAWxp1J0mD1Fle4d")
############## Collection of Tweets ###################
## Searching for tweets ##
search.string <- "#deletefacebook"
no.of.tweets <- 100
tweets <- searchTwitter(search.string, n=no.of.tweets, lang="en", since= Start_date , until = End_date)
##rm(list =ls())
#### Code to collect tweets##
## All tweets collected are filter and appended to csv files in data folder automatically
library(twitteR)
library(ggplot2)
library(ggmap)
library(data.table)
library(stringr)
Start_date <- "2018-03-20"
End_date <- "2018-03-29"
## Setup oauth
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A",
"owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m",
"340449785-0AWt3nkBVvLlX7hbUFLl0fEqIKs47qUU7V5UnFWH",
"qnaD0Pyp9jUXfwVb82RlSKikuvVi2MAWxp1J0mD1Fle4d")
############## Collection of Tweets ###################
## Searching for tweets ##
search.string <- "#deletefacebook"
no.of.tweets <- 100
tweets <- searchTwitter(search.string, n=no.of.tweets, lang="en", since= Start_date , until = End_date)
##rm(list =ls())
#### Code to collect tweets##
## All tweets collected are filter and appended to csv files in data folder automatically
library(twitteR)
library(ggplot2)
library(ggmap)
library(data.table)
library(stringr)
Start_date <- "2018-03-20"
End_date <- "2018-03-29"
## Setup oauth
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A",
"owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m",
"340449785-0AWt3nkBVvLlX7hbUFLl0fEqIKs47qUU7V5UnFWH",
"qnaD0Pyp9jUXfwVb82RlSKikuvVi2MAWxp1J0mD1Fle4d")
############## Collection of Tweets ###################
## Searching for tweets ##
search.string <- c("cambridge","analytica")
no.of.tweets <- 2500
tweets <- searchTwitter(search.string, n=no.of.tweets, lang="en", since= Start_date , until = End_date)
## Conversion of searched tweets to Data frame
tweets <- twListToDF(tweets)
View(tweets)
View(tweets)
Tweets_Collected_prepocessed <- tweets
## Remove non- ASCII characters, hastags (#xxxxx) used in tweet search, tags(@xxxxxx) and other special characters
Tweets_Collected_prepocessed <- (iconv(Tweets_Collected$text, "latin1", "ASCII", sub=""))
Tweets_Collected <- tweets
## Remove non- ASCII characters, hastags (#xxxxx) used in tweet search, tags(@xxxxxx) and other special characters
Tweets_Collected_prepocessed <- (iconv(Tweets_Collected$text, "latin1", "ASCII", sub=""))
Tweets_Collected_prepocessed <- sub("#\\w+ *", "", Tweets_Collected_prepocessed)
Tweets_Collected_prepocessed <- data.frame(sub("@\\w+ *", "", Tweets_Collected_prepocessed))
View(Tweets_Collected_prepocessed)
Tweets_Collected_prepocessed <- data.frame(sub("@\\w+ *", "", Tweets_Collected_prepocessed))
## Remove non- ASCII characters, hastags (#xxxxx) used in tweet search, tags(@xxxxxx) and other special characters
Tweets_Collected_prepocessed <- (iconv(Tweets_Collected$text, "latin1", "ASCII", sub=""))
Tweets_Collected_prepocessed <- sub("#\\w+ *", "", Tweets_Collected_prepocessed)
Tweets_Collected_prepocessed <- data.frame(sub("@\\w+ *", "", Tweets_Collected_prepocessed))
View(Tweets_Collected_prepocessed)
Start_date <- "2018-03-20"
End_date <- "2018-03-22"
## Setup oauth
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A",
"owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m",
"340449785-0AWt3nkBVvLlX7hbUFLl0fEqIKs47qUU7V5UnFWH",
"qnaD0Pyp9jUXfwVb82RlSKikuvVi2MAWxp1J0mD1Fle4d")
############## Collection of Tweets ###################
## Searching for tweets ##
search.string <- c("cambridge","analytica")
no.of.tweets <- 2500
tweets <- searchTwitter(search.string, n=no.of.tweets, lang="en", since= Start_date , until = End_date)
Start_date <- "2018-03-20"
End_date <- "2018-03-25"
## Setup oauth
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A",
"owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m",
"340449785-0AWt3nkBVvLlX7hbUFLl0fEqIKs47qUU7V5UnFWH",
"qnaD0Pyp9jUXfwVb82RlSKikuvVi2MAWxp1J0mD1Fle4d")
############## Collection of Tweets ###################
## Searching for tweets ##
search.string <- c("cambridge","analytica")
no.of.tweets <- 2500
tweets <- searchTwitter(search.string, n=no.of.tweets, lang="en", since= Start_date , until = End_date)
setwd("~/Documents/GitHub/Data-Integration-Big-Data-Analysis-And-Visualization/Part2/code")
setwd("../Data/Day_1/Tweets")
setwd("../Data/Day_1/Tweets")
write.table(Tweets_Collected_prepocessed,"March 20 - Tweets",  sep="\t", col.names = F, row.names = F)
library(rtimes)
Start_Date <- "20180310"
End_Data <- "20180331"
## Setting up Authentication
Sys.setenv(NYTIMES_AS_KEY = "fa567ce571174336957fc6786b4dc91e")
## Collecting of articles
DF <- as_search(q = "cambridge","analytica" , begin_date = Start_Date, end_date = End_Data, all_results = TRUE)
## Pre- processing ( removing duplicates)
Data <- DF$data
Data <- subset(Data, select = -c(multimedia, keywords, byline.person))
Data = Data[!duplicated(Data$`_id`),]
Data = Data[!duplicated(Data$snippet),]
Data = Data[!duplicated(Data$word_count),]
## Saving the articles collected for the day
setwd("../Data/NYTimes")
Name=paste("NYTimes","- Collected" ,nrow(Data)," articles on :",Sys.time())
write.csv(Data, Name)
setwd("../../code")
## Read all articles collected so far
setwd("../Data")
Articles <- read.csv("NYTimes_Articles_Info")
Articles <- subset(Articles, select = -c(X))
names(Articles)[names(Articles) == 'X_id'] <- '_id'
## Merge collected articles with existing articles and saving a consolidated csv file
Articles <- rbind(Articles,Data)
Articles = Articles[!duplicated(Articles$`_id`),]
Articles = Articles[!duplicated(Articles$snippet),]
Articles = Articles[!duplicated(Articles$word_count),]
write.csv(Articles, "NYTimes_Articles_Info")
setwd("../code")
setwd("~/Documents/GitHub/Data-Integration-Big-Data-Analysis-And-Visualization/Part2/code")
library(rtimes)
Start_Date <- "20180310"
End_Data <- "20180331"
## Setting up Authentication
Sys.setenv(NYTIMES_AS_KEY = "fa567ce571174336957fc6786b4dc91e")
## Collecting of articles
DF <- as_search(q = "cambridge","analytica" , begin_date = Start_Date, end_date = End_Data, all_results = TRUE)
## Pre- processing ( removing duplicates)
Data <- DF$data
Data <- subset(Data, select = -c(multimedia, keywords, byline.person))
Data = Data[!duplicated(Data$`_id`),]
Data = Data[!duplicated(Data$snippet),]
Data = Data[!duplicated(Data$word_count),]
## Saving the articles collected for the day
setwd("../Data/NYTimes")
Name=paste("NYTimes","- Collected" ,nrow(Data)," articles on :",Sys.time())
write.csv(Data, Name)
setwd("../../code")
## Read all articles collected so far
setwd("../Data")
Articles <- read.csv("NYTimes_Articles_Info")
Articles <- subset(Articles, select = -c(X))
names(Articles)[names(Articles) == 'X_id'] <- '_id'
## Merge collected articles with existing articles and saving a consolidated csv file
Articles <- rbind(Articles,Data)
Articles = Articles[!duplicated(Articles$`_id`),]
Articles = Articles[!duplicated(Articles$snippet),]
Articles = Articles[!duplicated(Articles$word_count),]
write.csv(Articles, "NYTimes_Articles_Info")
setwd("../code")
setwd("~/Documents/GitHub/Data-Integration-Big-Data-Analysis-And-Visualization/Part2/code")
## Read all articles collected so far
setwd("../Data")
Articles <- read.csv("NYTimes_Articles_Info")
library('RCurl')
library('XML')
library(Rcrawler)
library(stringr)
## Read all articles collected so far
setwd("../Data")
Articles <- read.csv("NYTimes_Articles_Info")
Articles <- subset(Articles, select = -c(X))
setwd("../code")
#Rcrawler(Website = "https://www.nytimes.com/aponline/2018/03/20/world/europe/ap-eu-britain-facebook-cambridge-analytica-the-latest.html", no_cores = 4, no_conn = 4, ExtractXpathPat = c("//h1","//article"), PatternsNames = c("Title","Content"))
## Using rcrawler to extract the article from URL (present in "Articles")
## try is used for error handling
## A total word count is done on the article to elimninate duplicates
for (i in c(1:nrow(Articles))){
if (i==1){
try(Data<-ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
content <-data.frame(Data)
content$Content <- str_replace_all(content$Content, "[^[:alnum:]]", " ")  ## removes special characters
wordcount <- length(unlist(strsplit(toString(content$Content[i])," "))) ## gets a count of the number of words in the article
content <- cbind(content,wordcount)
}
if (i>1){
try(Data <- ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
temp <- data.frame(Data)
temp$Content <- str_replace_all(temp$Content, "[^[:alnum:]]", " ") ##removes special characters
wordcount <- length(unlist(strsplit(toString(temp$Content)," "))) ## gets a count of the number of words in the article
temp <- cbind(temp,wordcount)
content <- rbind(content,temp)
}
print(i)
}
## Eliminating duplicates caused by error handling
content = content[!duplicated(content$wordcount),]
## Saving the article extracted
setwd("../Data")
write.csv(content,"NYTimes_Articles_data")
setwd("../code")
View(content)
sum(content$Content)
sum(content$wordcount)
##rm(list =ls())
#### Code to collect tweets##
## All tweets collected are filter and appended to csv files in data folder automatically
library(twitteR)
library(ggplot2)
library(ggmap)
library(data.table)
library(stringr)
Start_date <- "2018-03-10"
End_date <- "2018-03-31"
## Setup oauth
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A",
"owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m",
"340449785-0AWt3nkBVvLlX7hbUFLl0fEqIKs47qUU7V5UnFWH",
"qnaD0Pyp9jUXfwVb82RlSKikuvVi2MAWxp1J0mD1Fle4d")
############## Collection of Tweets ###################
## Searching for tweets ##
search.string <- c("cambridge","analytica")
no.of.tweets <- 20000
tweets <- searchTwitter(search.string, n=no.of.tweets, lang="en", since= Start_date , until = End_date)
##rm(list =ls())
#### Code to collect tweets##
## All tweets collected are filter and appended to csv files in data folder automatically
library(twitteR)
library(ggplot2)
library(ggmap)
library(data.table)
library(stringr)
Start_date <- "2018-03-10"
End_date <- "2018-03-31"
## Setup oauth
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A",
"owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m",
"340449785-0AWt3nkBVvLlX7hbUFLl0fEqIKs47qUU7V5UnFWH",
"qnaD0Pyp9jUXfwVb82RlSKikuvVi2MAWxp1J0mD1Fle4d")
############## Collection of Tweets ###################
## Searching for tweets ##
search.string <- c("cambridge","analytica")
no.of.tweets <- 1000
tweets <- searchTwitter(search.string, n=no.of.tweets, lang="en", since= Start_date , until = End_date)
