#content <- cbind(content,wordcount)
}
if (i>1){
Data<-possibly(ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")),
NULL,
quiet = FALSE)
temp <- data.frame(Data)
wordcount <- length(unlist(strsplit(toString(temp$Content[1])," ")))
#temp <- cbind(temp,wordcount)
content <- rbind(content,temp)
}
print(i)
}
library('RCurl')
library('XML')
library(Rcrawler)
## Read all articles collected so far
setwd("../Data")
Articles <- read.csv("NYTimes_Articles_Collected")
Articles <- subset(Articles, select = -c(X))
setwd("../code")
## Extracting url from data collected
#url <- toString(data.frame(Articles$web_url)[1,])
#pageToRead <- readLines(url)
#grep('Opponent / Event',pageToRead)
#mypattern = '<td class="row-text">([^<]*)</td>'
#datalines = grep(mypattern,pageToRead,value=TRUE)
## Extract information without HTML tags
#getexpr = function(s,g)substring(s,g,g+attr(g,'match.length')-1)
#gg = gregexpr(mypattern,pageToRead)
#matches = mapply(getexpr,pageToRead,gg)
#result = gsub(mypattern,'\\1',matches)
#names(result) = NULL
#Rcrawler(Website = "https://www.nytimes.com/aponline/2018/03/20/world/europe/ap-eu-britain-facebook-cambridge-analytica-the-latest.html", no_cores = 4, no_conn = 4, ExtractXpathPat = c("//h1","//article"), PatternsNames = c("Title","Content"))
for (i in c(1:length(Articles))){
if (i==1){
Data<-ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content"))
content <-data.frame(Data)
wordcount <- length(unlist(strsplit(toString(content$Content[1])," ")))
content <- cbind(content,wordcount)
}
if (i>1){
is.error(ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
next
Data <- ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content"))
temp <- data.frame(Data)
wordcount <- length(unlist(strsplit(toString(temp$Content[1])," ")))
temp <- cbind(temp,wordcount)
content <- rbind(content,temp)
}
print(i)
}
library('RCurl')
library('XML')
library(Rcrawler)
## Read all articles collected so far
setwd("../Data")
Articles <- read.csv("NYTimes_Articles_Collected")
Articles <- subset(Articles, select = -c(X))
setwd("../code")
## Extracting url from data collected
#url <- toString(data.frame(Articles$web_url)[1,])
#pageToRead <- readLines(url)
#grep('Opponent / Event',pageToRead)
#mypattern = '<td class="row-text">([^<]*)</td>'
#datalines = grep(mypattern,pageToRead,value=TRUE)
## Extract information without HTML tags
#getexpr = function(s,g)substring(s,g,g+attr(g,'match.length')-1)
#gg = gregexpr(mypattern,pageToRead)
#matches = mapply(getexpr,pageToRead,gg)
#result = gsub(mypattern,'\\1',matches)
#names(result) = NULL
#Rcrawler(Website = "https://www.nytimes.com/aponline/2018/03/20/world/europe/ap-eu-britain-facebook-cambridge-analytica-the-latest.html", no_cores = 4, no_conn = 4, ExtractXpathPat = c("//h1","//article"), PatternsNames = c("Title","Content"))
for (i in c(1:length(Articles))){
if (i==1){
Data<-ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content"))
content <-data.frame(Data)
wordcount <- length(unlist(strsplit(toString(content$Content[1])," ")))
content <- cbind(content,wordcount)
}
if (i>1){
is.error(ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
{next}
Data <- ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content"))
temp <- data.frame(Data)
wordcount <- length(unlist(strsplit(toString(temp$Content[1])," ")))
temp <- cbind(temp,wordcount)
content <- rbind(content,temp)
}
print(i)
}
library('RCurl')
library('XML')
library(Rcrawler)
## Read all articles collected so far
setwd("../Data")
Articles <- read.csv("NYTimes_Articles_Collected")
Articles <- subset(Articles, select = -c(X))
setwd("../code")
## Extracting url from data collected
#url <- toString(data.frame(Articles$web_url)[1,])
#pageToRead <- readLines(url)
#grep('Opponent / Event',pageToRead)
#mypattern = '<td class="row-text">([^<]*)</td>'
#datalines = grep(mypattern,pageToRead,value=TRUE)
## Extract information without HTML tags
#getexpr = function(s,g)substring(s,g,g+attr(g,'match.length')-1)
#gg = gregexpr(mypattern,pageToRead)
#matches = mapply(getexpr,pageToRead,gg)
#result = gsub(mypattern,'\\1',matches)
#names(result) = NULL
#Rcrawler(Website = "https://www.nytimes.com/aponline/2018/03/20/world/europe/ap-eu-britain-facebook-cambridge-analytica-the-latest.html", no_cores = 4, no_conn = 4, ExtractXpathPat = c("//h1","//article"), PatternsNames = c("Title","Content"))
for (i in c(1:length(Articles))){
if (i==1){
try(Data<-ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
content <-data.frame(Data)
wordcount <- length(unlist(strsplit(toString(content$Content[1])," ")))
content <- cbind(content,wordcount)
}
if (i>1){
try(Data <- ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
temp <- data.frame(Data)
wordcount <- length(unlist(strsplit(toString(temp$Content[1])," ")))
temp <- cbind(temp,wordcount)
content <- rbind(content,temp)
}
print(i)
}
length(Articles)
nrow(Articles)
View(content)
View(content)
library('RCurl')
library('XML')
library(Rcrawler)
## Read all articles collected so far
setwd("../Data")
Articles <- read.csv("NYTimes_Articles_Collected")
Articles <- subset(Articles, select = -c(X))
setwd("../code")
## Extracting url from data collected
#url <- toString(data.frame(Articles$web_url)[1,])
#pageToRead <- readLines(url)
#grep('Opponent / Event',pageToRead)
#mypattern = '<td class="row-text">([^<]*)</td>'
#datalines = grep(mypattern,pageToRead,value=TRUE)
## Extract information without HTML tags
#getexpr = function(s,g)substring(s,g,g+attr(g,'match.length')-1)
#gg = gregexpr(mypattern,pageToRead)
#matches = mapply(getexpr,pageToRead,gg)
#result = gsub(mypattern,'\\1',matches)
#names(result) = NULL
#Rcrawler(Website = "https://www.nytimes.com/aponline/2018/03/20/world/europe/ap-eu-britain-facebook-cambridge-analytica-the-latest.html", no_cores = 4, no_conn = 4, ExtractXpathPat = c("//h1","//article"), PatternsNames = c("Title","Content"))
for (i in c(1:nrow(Articles))){
if (i==1){
try(Data<-ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
content <-data.frame(Data)
wordcount <- length(unlist(strsplit(toString(content$Content[1])," ")))
content <- cbind(content,wordcount)
}
if (i>1){
try(Data <- ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
temp <- data.frame(Data)
wordcount <- length(unlist(strsplit(toString(temp$Content[1])," ")))
temp <- cbind(temp,wordcount)
content <- rbind(content,temp)
}
print(i)
}
content = content[!duplicated(content$wordcount),]
## Savind the article extracted
setwd("../Data")
write.csv(content,"NYTimes_Article_data")
setwd("../code")
library(rtimes)
## Setting up Authentication
Sys.setenv(NYTIMES_AS_KEY = "fa567ce571174336957fc6786b4dc91e")
## Collecting of articles
DF <- as_search(q = "delete","facebook" , begin_date = "20180310", end_date = "20180329", all_results = TRUE)
## Pre- processing ( removing duplicates)
Data <- DF$data
Data <- subset(Data, select = -c(multimedia, keywords, byline.person))
Data = Data[!duplicated(Data$`_id`),]
Data = Data[!duplicated(Data$snippet),]
Data = Data[!duplicated(Data$word_count),]
## Saving the articles collected for the day
setwd("../Data/NYTimes")
Name=paste("NYTimes","- Collected" ,nrow(Data)," articles on :",Sys.time())
write.csv(Data, Name)
setwd("../../code")
## Read all articles collected so far
setwd("../Data")
Articles <- read.csv("NYTimes_Articles_Info")
Articles <- subset(Articles, select = -c(X))
names(Articles)[names(Articles) == 'X_id'] <- '_id'
## Merge collected articles with existing articles and saving a consolidated csv file
Articles <- rbind(Articles,Data)
Articles = Articles[!duplicated(Articles$`_id`),]
Articles = Articles[!duplicated(Articles$snippet),]
Articles = Articles[!duplicated(Articles$word_count),]
write.csv(Articles, "NYTimes_Articles_Info")
setwd("../code")
library(rtimes)
## Setting up Authentication
Sys.setenv(NYTIMES_AS_KEY = "fa567ce571174336957fc6786b4dc91e")
## Collecting of articles
DF <- as_search(q = "cambridge","analytica" , begin_date = "20180310", end_date = "20180329", all_results = TRUE)
library(rtimes)
## Setting up Authentication
Sys.setenv(NYTIMES_AS_KEY = "fa567ce571174336957fc6786b4dc91e")
## Collecting of articles
DF <- as_search(q = "cambridge","analytica" , begin_date = "20180310", end_date = "20180329", all_results = TRUE)
## Pre- processing ( removing duplicates)
Data <- DF$data
Data <- subset(Data, select = -c(multimedia, keywords, byline.person))
Data = Data[!duplicated(Data$`_id`),]
Data = Data[!duplicated(Data$snippet),]
Data = Data[!duplicated(Data$word_count),]
## Saving the articles collected for the day
setwd("../Data/NYTimes")
Name=paste("NYTimes","- Collected" ,nrow(Data)," articles on :",Sys.time())
write.csv(Data, Name)
setwd("../../code")
## Read all articles collected so far
setwd("../Data")
Articles <- read.csv("NYTimes_Articles_Info")
Articles <- subset(Articles, select = -c(X))
names(Articles)[names(Articles) == 'X_id'] <- '_id'
## Merge collected articles with existing articles and saving a consolidated csv file
Articles <- rbind(Articles,Data)
Articles = Articles[!duplicated(Articles$`_id`),]
Articles = Articles[!duplicated(Articles$snippet),]
Articles = Articles[!duplicated(Articles$word_count),]
write.csv(Articles, "NYTimes_Articles_Info")
setwd("../code")
View(Articles)
library('RCurl')
library('XML')
library(Rcrawler)
## Read all articles collected so far
setwd("../Data")
Articles <- read.csv("NYTimes_Articles_Collected")
Articles <- subset(Articles, select = -c(X))
setwd("../code")
#Rcrawler(Website = "https://www.nytimes.com/aponline/2018/03/20/world/europe/ap-eu-britain-facebook-cambridge-analytica-the-latest.html", no_cores = 4, no_conn = 4, ExtractXpathPat = c("//h1","//article"), PatternsNames = c("Title","Content"))
## Using rcrawler to extract the article from URL (present in "Articles")
## try is used for error handling
## A total word count is done on the article to elimninate duplicates
for (i in c(1:nrow(Articles))){
if (i==1){
try(Data<-ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
content <-data.frame(Data)
wordcount <- length(unlist(strsplit(toString(content$Content[1])," ")))
content <- cbind(content,wordcount)
}
if (i>1){
try(Data <- ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
temp <- data.frame(Data)
wordcount <- length(unlist(strsplit(toString(temp$Content[1])," ")))
temp <- cbind(temp,wordcount)
content <- rbind(content,temp)
}
print(i)
}
## Eliminating duplicates caused by error handling
content = content[!duplicated(content$wordcount),]
## Saving the article extracted
setwd("../Data")
write.csv(content,"NYTimes_Articles_data")
setwd("../code")
View(content)
sum(content$wordcount)
View(content)
content$Content[1]
View(Articles)
a <- str_replace_all(content$Content[1], "[^[:alnum:]]", " ")
a
a <- str_replace_all(content$Content[1], "[^[:alnum:]]", "")
a
a <- str_replace_all(content$Content[1], "[^[:alnum:]]", "")
a <- str_replace_all(content$Content[1], "[^[:alnum:]]", " ")
a
a <- str_replace_all(content$Content[1], "  ", " ")
a
a <- str_replace_all(content$Content[1], "[^[:alnum:]]", " ")
a <- str_replace_all(a, "[^[:alnum:]]", " ")
a
b <- str_replace_all(a, "[^[:alnum:]]", " ")
b <- str_replace_all(a, "  ", " ")
b
b <- str_replace_all(a, "             ", " ")
b
b <- str_replace_all(a, "             ", " ")
b <- str_replace_all(a, "             ", " ")
b <- str_replace_all(a, "    ", " ")
b <- str_replace_all(a, "  ", " ")
b
a
library('RCurl')
library('XML')
library(Rcrawler)
## Read all articles collected so far
setwd("../Data")
Articles <- read.csv("NYTimes_Articles_Collected")
Articles <- subset(Articles, select = -c(X))
setwd("../code")
#Rcrawler(Website = "https://www.nytimes.com/aponline/2018/03/20/world/europe/ap-eu-britain-facebook-cambridge-analytica-the-latest.html", no_cores = 4, no_conn = 4, ExtractXpathPat = c("//h1","//article"), PatternsNames = c("Title","Content"))
## Using rcrawler to extract the article from URL (present in "Articles")
## try is used for error handling
## A total word count is done on the article to elimninate duplicates
for (i in c(1:nrow(Articles))){
if (i==1){
try(Data<-ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
content <-data.frame(Data)
content <- str_replace_all(content, "[^[:alnum:]]", " ")  ## removes special characters
wordcount <- length(unlist(strsplit(toString(content$Content[1])," "))) ## gets a count of the number of words in the article
content <- cbind(content,wordcount)
}
if (i>1){
try(Data <- ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
temp <- data.frame(Data)
temp <- str_replace_all(temp, "[^[:alnum:]]", " ") ##removes special characters
wordcount <- length(unlist(strsplit(toString(temp$Content[1])," "))) ## gets a count of the number of words in the article
temp <- cbind(temp,wordcount)
content <- rbind(content,temp)
}
print(i)
}
## Eliminating duplicates caused by error handling
content = content[!duplicated(content$wordcount),]
## Saving the article extracted
setwd("../Data")
write.csv(content,"NYTimes_Articles_data")
setwd("../code")
library('RCurl')
library('XML')
library(Rcrawler)
## Read all articles collected so far
setwd("../Data")
Articles <- read.csv("NYTimes_Articles_Collected")
Articles <- subset(Articles, select = -c(X))
setwd("../code")
#Rcrawler(Website = "https://www.nytimes.com/aponline/2018/03/20/world/europe/ap-eu-britain-facebook-cambridge-analytica-the-latest.html", no_cores = 4, no_conn = 4, ExtractXpathPat = c("//h1","//article"), PatternsNames = c("Title","Content"))
## Using rcrawler to extract the article from URL (present in "Articles")
## try is used for error handling
## A total word count is done on the article to elimninate duplicates
for (i in c(1:nrow(Articles))){
if (i==1){
try(Data<-ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
content <-data.frame(Data)
content <- str_replace_all(content, "[^[:alnum:]]", " ")  ## removes special characters
wordcount <- length(unlist(strsplit(toString(content$Content[1])," "))) ## gets a count of the number of words in the article
content <- cbind(content,wordcount)
}
if (i>1){
try(Data <- ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
temp <- data.frame(Data)
temp <- str_replace_all(temp, "[^[:alnum:]]", " ") ##removes special characters
wordcount <- length(unlist(strsplit(toString(temp$Content[1])," "))) ## gets a count of the number of words in the article
temp <- cbind(temp,wordcount)
content <- rbind(content,temp)
}
print(i)
}
## Eliminating duplicates caused by error handling
content = content[!duplicated(content$wordcount),]
## Saving the article extracted
setwd("../Data")
write.csv(content,"NYTimes_Articles_data")
setwd("../code")
setwd("~/Documents/GitHub/Data-Integration-Big-Data-Analysis-And-Visualization/Part2/code")
library('RCurl')
library('XML')
library(Rcrawler)
## Read all articles collected so far
setwd("../Data")
Articles <- read.csv("NYTimes_Articles_Collected")
Articles <- subset(Articles, select = -c(X))
setwd("../code")
#Rcrawler(Website = "https://www.nytimes.com/aponline/2018/03/20/world/europe/ap-eu-britain-facebook-cambridge-analytica-the-latest.html", no_cores = 4, no_conn = 4, ExtractXpathPat = c("//h1","//article"), PatternsNames = c("Title","Content"))
## Using rcrawler to extract the article from URL (present in "Articles")
## try is used for error handling
## A total word count is done on the article to elimninate duplicates
for (i in c(1:nrow(Articles))){
if (i==1){
try(Data<-ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
content <-data.frame(Data)
content <- str_replace_all(content, "[^[:alnum:]]", " ")  ## removes special characters
wordcount <- length(unlist(strsplit(toString(content$Content[1])," "))) ## gets a count of the number of words in the article
content <- cbind(content,wordcount)
}
if (i>1){
try(Data <- ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
temp <- data.frame(Data)
temp <- str_replace_all(temp, "[^[:alnum:]]", " ") ##removes special characters
wordcount <- length(unlist(strsplit(toString(temp$Content[1])," "))) ## gets a count of the number of words in the article
temp <- cbind(temp,wordcount)
content <- rbind(content,temp)
}
print(i)
}
## Eliminating duplicates caused by error handling
content = content[!duplicated(content$wordcount),]
## Saving the article extracted
setwd("../Data")
write.csv(content,"NYTimes_Articles_data")
setwd("../code")
library('RCurl')
library('XML')
library(Rcrawler)
## Read all articles collected so far
setwd("../Data")
Articles <- read.csv("NYTimes_Articles_Collected")
Articles <- subset(Articles, select = -c(X))
setwd("../code")
#Rcrawler(Website = "https://www.nytimes.com/aponline/2018/03/20/world/europe/ap-eu-britain-facebook-cambridge-analytica-the-latest.html", no_cores = 4, no_conn = 4, ExtractXpathPat = c("//h1","//article"), PatternsNames = c("Title","Content"))
## Using rcrawler to extract the article from URL (present in "Articles")
## try is used for error handling
## A total word count is done on the article to elimninate duplicates
i=1
setwd("~/Documents/GitHub/Data-Integration-Big-Data-Analysis-And-Visualization/Part2/code")
library('RCurl')
library('XML')
library(Rcrawler)
## Read all articles collected so far
setwd("../Data")
Articles <- read.csv("NYTimes_Articles_Collected")
Articles <- subset(Articles, select = -c(X))
setwd("../code")
## Read all articles collected so far
setwd("../Data")
library('RCurl')
library('XML')
library(Rcrawler)
## Read all articles collected so far
setwd("../Data")
Articles <- read.csv("NYTimes_Articles_Info")
Articles <- subset(Articles, select = -c(X))
setwd("../code")
## Using rcrawler to extract the article from URL (present in "Articles")
## try is used for error handling
## A total word count is done on the article to elimninate duplicates
for (i in c(1:nrow(Articles))){
if (i==1){
try(Data<-ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
content <-data.frame(Data)
content <- str_replace_all(content, "[^[:alnum:]]", " ")  ## removes special characters
wordcount <- length(unlist(strsplit(toString(content$Content[1])," "))) ## gets a count of the number of words in the article
content <- cbind(content,wordcount)
}
if (i>1){
try(Data <- ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
temp <- data.frame(Data)
temp <- str_replace_all(temp, "[^[:alnum:]]", " ") ##removes special characters
wordcount <- length(unlist(strsplit(toString(temp$Content[1])," "))) ## gets a count of the number of words in the article
temp <- cbind(temp,wordcount)
content <- rbind(content,temp)
}
print(i)
}
## Eliminating duplicates caused by error handling
content = content[!duplicated(content$wordcount),]
## Saving the article extracted
setwd("../Data")
write.csv(content,"NYTimes_Articles_data")
setwd("../code")
try(Data<-ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
content <-data.frame(Data)
content <- str_replace_all(content, "[^[:alnum:]]", " ")  ## removes special characters
wordcount <- length(unlist(strsplit(toString(content$Content[1])," "))) ## gets a count of the number of words in the article
content
try(Data<-ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
content <-data.frame(Data)
content$Content <- str_replace_all(content$Content, "[^[:alnum:]]", " ")  ## removes special characters
View(content)
library('RCurl')
library('XML')
library(Rcrawler)
## Read all articles collected so far
setwd("../Data")
Articles <- read.csv("NYTimes_Articles_Info")
Articles <- subset(Articles, select = -c(X))
setwd("../code")
#Rcrawler(Website = "https://www.nytimes.com/aponline/2018/03/20/world/europe/ap-eu-britain-facebook-cambridge-analytica-the-latest.html", no_cores = 4, no_conn = 4, ExtractXpathPat = c("//h1","//article"), PatternsNames = c("Title","Content"))
## Using rcrawler to extract the article from URL (present in "Articles")
## try is used for error handling
## A total word count is done on the article to elimninate duplicates
for (i in c(1:nrow(Articles))){
if (i==1){
try(Data<-ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
content <-data.frame(Data)
content$Content <- str_replace_all(content$Content, "[^[:alnum:]]", " ")  ## removes special characters
wordcount <- length(unlist(strsplit(toString(content$Content[1])," "))) ## gets a count of the number of words in the article
content <- cbind(content,wordcount)
}
if (i>1){
try(Data <- ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
temp <- data.frame(Data)
temp$Content <- str_replace_all(temp$Content, "[^[:alnum:]]", " ") ##removes special characters
wordcount <- length(unlist(strsplit(toString(temp$Content[1])," "))) ## gets a count of the number of words in the article
temp <- cbind(temp,wordcount)
content <- rbind(content,temp)
}
print(i)
}
## Eliminating duplicates caused by error handling
content = content[!duplicated(content$wordcount),]
## Saving the article extracted
setwd("../Data")
write.csv(content,"NYTimes_Articles_data")
setwd("../code")
content$Content[1]
