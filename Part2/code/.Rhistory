library(rtimes)
Start_Date <- "20180310"
End_Data <- "20180305"
## Setting up Authentication
Sys.setenv(NYTIMES_AS_KEY = "fa567ce571174336957fc6786b4dc91e")
## Collecting of articles
DF <- as_search(q = "facebook scandal", begin_date = Start_Date, end_date = End_Data, all_results = TRUE)
## Pre- processing ( removing duplicates)
Data <- DF$data
Data <- subset(Data, select = -c(multimedia, keywords, byline.person))
Data = Data[!duplicated(Data$`_id`),]
Data = Data[!duplicated(Data$snippet),]
Data = Data[!duplicated(Data$word_count),]
## Saving the articles collected for the day
setwd("../Data/NYTimes")
Name=paste("NYTimes","- Collected" ,nrow(Data)," articles on :",Sys.time())
write.csv(Data, Name)
setwd("../../code")
## Read all articles collected so far
setwd("../Data/NYTimes")
Articles <- read.csv("NYTimes_Articles_Collected")
Articles <- subset(Articles, select = -c(X))
names(Articles)[names(Articles) == 'X_id'] <- '_id'
## Merge collected articles with existing articles and saving a consolidated csv file
Articles <- rbind(Articles,Data)
Articles = Articles[!duplicated(Articles$`_id`),]
Articles = Articles[!duplicated(Articles$snippet),]
Articles = Articles[!duplicated(Articles$word_count),]
write.csv(Articles, "NYTimes_Articles_Collected")
setwd("../../code")
library(rtimes)
Start_Date <- "20180310"
End_Data <- "20180405"
## Setting up Authentication
Sys.setenv(NYTIMES_AS_KEY = "fa567ce571174336957fc6786b4dc91e")
## Collecting of articles
DF <- as_search(q = "facebook scandal", begin_date = Start_Date, end_date = End_Data, all_results = TRUE)
## Pre- processing ( removing duplicates)
Data <- DF$data
Data <- subset(Data, select = -c(multimedia, keywords, byline.person))
Data = Data[!duplicated(Data$`_id`),]
Data = Data[!duplicated(Data$snippet),]
Data = Data[!duplicated(Data$word_count),]
## Saving the articles collected for the day
setwd("../Data/NYTimes")
Name=paste("NYTimes","- Collected" ,nrow(Data)," articles on :",Sys.time())
write.csv(Data, Name)
setwd("../../code")
## Read all articles collected so far
setwd("../Data/NYTimes")
Articles <- read.csv("NYTimes_Articles_Collected")
Articles <- subset(Articles, select = -c(X))
names(Articles)[names(Articles) == 'X_id'] <- '_id'
## Merge collected articles with existing articles and saving a consolidated csv file
Articles <- rbind(Articles,Data)
Articles = Articles[!duplicated(Articles$`_id`),]
Articles = Articles[!duplicated(Articles$snippet),]
Articles = Articles[!duplicated(Articles$word_count),]
write.csv(Articles, "NYTimes_Articles_Collected")
setwd("../../code")
View(Articles)
library(rtimes)
Start_Date <- "20180310"
End_Data <- "20180405"
## Setting up Authentication
Sys.setenv(NYTIMES_AS_KEY = "fa567ce571174336957fc6786b4dc91e")
## Collecting of articles
DF <- as_search(q = "facebook", "scandal", begin_date = Start_Date, end_date = End_Data, all_results = TRUE)
## Pre- processing ( removing duplicates)
Data <- DF$data
Data <- subset(Data, select = -c(multimedia, keywords, byline.person))
Data = Data[!duplicated(Data$`_id`),]
Data = Data[!duplicated(Data$snippet),]
Data = Data[!duplicated(Data$word_count),]
## Collecting of articles
DF <- as_search(q = "facebook", "scandal", begin_date = Start_Date, end_date = End_Data, all_results = TRUE)
## Pre- processing ( removing duplicates)
Data <- DF$data
Data <- subset(Data, select = -c(multimedia, keywords, byline.person))
Data = Data[!duplicated(Data$`_id`),]
Data = Data[!duplicated(Data$snippet),]
Data = Data[!duplicated(Data$word_count),]
## Saving the articles collected for the day
setwd("../Data/NYTimes")
Name=paste("NYTimes","- Collected" ,nrow(Data)," articles on :",Sys.time())
write.csv(Data, Name)
setwd("../../code")
## Read all articles collected so far
setwd("../Data/NYTimes")
Articles <- read.csv("NYTimes_Articles_Collected")
Articles <- subset(Articles, select = -c(X))
names(Articles)[names(Articles) == 'X_id'] <- '_id'
## Merge collected articles with existing articles and saving a consolidated csv file
Articles <- rbind(Articles,Data)
Articles = Articles[!duplicated(Articles$`_id`),]
Articles = Articles[!duplicated(Articles$snippet),]
Articles = Articles[!duplicated(Articles$word_count),]
write.csv(Articles, "NYTimes_Articles_Collected")
setwd("../../code")
Articles<-Data
names(Articles)[names(Articles) == 'X_id'] <- '_id'
## Merge collected articles with existing articles and saving a consolidated csv file
Articles <- rbind(Articles,Data)
Articles = Articles[!duplicated(Articles$`_id`),]
Articles = Articles[!duplicated(Articles$snippet),]
Articles = Articles[!duplicated(Articles$word_count),]
View(Articles)
write.csv(Articles, "NYTimes_Articles_Collected")
setwd("../../code")
## Saving the articles collected for the day
setwd("../Data/NYTimes")
Name=paste("NYTimes","- Collected" ,nrow(Data)," articles on :",Sys.time())
write.csv(Data, Name)
setwd("../../code")
## Read all articles collected so far
setwd("../Data/NYTimes")
Articles <- read.csv("NYTimes_Articles_Collected")
Data<- Articles
## Read all articles collected so far
setwd("../Data/NYTimes")
Articles <- read.csv("NYTimes_Articles_Total")
Articles <- subset(Articles, select = -c(X))
names(Articles)[names(Articles) == 'X_id'] <- '_id'
## Merge collected articles with existing articles and saving a consolidated csv file
Articles <- rbind(Articles,Data)
Articles = Articles[!duplicated(Articles$`_id`),]
Articles = Articles[!duplicated(Articles$snippet),]
Articles = Articles[!duplicated(Articles$word_count),]
write.csv(Articles, "NYTimes_Articles_Total")
setwd("../../code")
## Read all articles collected so far
setwd("../Data/NYTimes")
Articles <- read.csv("NYTimes_Articles_Total")
library('RCurl')
library('XML')
library(Rcrawler)
library(stringr)
## Read all articles collected so far
setwd("../Data")
setwd("~/Documents/GitHub/Data-Integration-Big-Data-Analysis-And-Visualization/Part2/code")
## Read all articles collected so far
setwd("../Data")
Articles <- read.csv("NYTimes_Articles_Total")
setwd("~/Documents/GitHub/Data-Integration-Big-Data-Analysis-And-Visualization/Part2/code")
## Read all articles collected so far
setwd("../Data/NYTimes")
Articles <- read.csv("NYTimes_Articles_Total")
Articles <- subset(Articles, select = -c(X))
setwd("../code")
setwd("../../code")
View(Articles)
## Saving the article extracted
setwd("../Data")
read.csv("NYTimes_Articles_data.txt")
a<-read.csv("NYTimes_Articles_data")
write.table(a,"articlesTotal.txt",col.names = F,row.names = F)
View(a)
setwd("~/Documents/GitHub/Data-Integration-Big-Data-Analysis-And-Visualization/Part2/code")
## Saving the article extracted
setwd("../Data")
a <- read.table("articlesTotal.txt")
a <- subset(a, select = -c(X))
View(a)
a <- subset(a, select = -c(V1))
View(a)
library('RCurl')
library('XML')
library(Rcrawler)
library(stringr)
## Read all articles collected so far
setwd("../Data/NYTimes")
Articles <- read.csv("NYTimes_Articles_Total")
Articles <- subset(Articles, select = -c(X))
setwd("../../code")
#Rcrawler(Website = "https://www.nytimes.com/aponline/2018/03/20/world/europe/ap-eu-britain-facebook-cambridge-analytica-the-latest.html", no_cores = 4, no_conn = 4, ExtractXpathPat = c("//h1","//article"), PatternsNames = c("Title","Content"))
## Using rcrawler to extract the article from URL (present in "Articles")
## try is used for error handling
## A total word count is done on the article to elimninate duplicates
for (i in c(1:nrow(Articles))){
if (i==1){
try(Data<-ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
content <-data.frame(Data)
content$Content <- str_replace_all(content$Content, "[^[:alnum:]]", " ")  ## removes special characters
wordcount <- length(unlist(strsplit(toString(content$Content[i])," "))) ## gets a count of the number of words in the article
content <- cbind(content,wordcount)
}
if (i>1){
try(Data <- ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
temp <- data.frame(Data)
temp$Content <- str_replace_all(temp$Content, "[^[:alnum:]]", " ") ##removes special characters
wordcount <- length(unlist(strsplit(toString(temp$Content)," "))) ## gets a count of the number of words in the article
temp <- cbind(temp,wordcount)
content <- rbind(content,temp)
}
print(i)
}
## Eliminating duplicates caused by error handling
content = content[!duplicated(content$wordcount),]
## Saving the article extracted
setwd("../Data")
write.table(content$Content,"articlesTotal.txt",col.names = F,row.names = F)
setwd("../code")
setwd("~/Documents/GitHub/Data-Integration-Big-Data-Analysis-And-Visualization/Part2/code")
library('RCurl')
library('XML')
library(Rcrawler)
library(stringr)
## Read all articles collected so far
setwd("../Data/NYTimes")
Articles <- read.csv("NYTimes_Articles_Total")
Articles <- subset(Articles, select = -c(X))
setwd("../../code")
#Rcrawler(Website = "https://www.nytimes.com/aponline/2018/03/20/world/europe/ap-eu-britain-facebook-cambridge-analytica-the-latest.html", no_cores = 4, no_conn = 4, ExtractXpathPat = c("//h1","//article"), PatternsNames = c("Title","Content"))
## Using rcrawler to extract the article from URL (present in "Articles")
## try is used for error handling
## A total word count is done on the article to elimninate duplicates
for (i in c(1:nrow(Articles))){
if (i==1){
try(Data<-ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
content <-data.frame(Data)
content$Content <- str_replace_all(content$Content, "[^[:alnum:]]", " ")  ## removes special characters
wordcount <- length(unlist(strsplit(toString(content$Content[i])," "))) ## gets a count of the number of words in the article
content <- cbind(content,wordcount)
}
if (i>1){
try(Data <- ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
temp <- data.frame(Data)
temp$Content <- str_replace_all(temp$Content, "[^[:alnum:]]", " ") ##removes special characters
wordcount <- length(unlist(strsplit(toString(temp$Content)," "))) ## gets a count of the number of words in the article
temp <- cbind(temp,wordcount)
content <- rbind(content,temp)
}
print(i)
}
## Eliminating duplicates caused by error handling
content = content[!duplicated(content$wordcount),]
## Saving the article extracted
setwd("../Data")
write.table(content$Content,"articlesTotal.txt",col.names = F,row.names = F)
setwd("../code")
try(Data<-ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
content <-data.frame(Data)
library('RCurl')
library('XML')
library(Rcrawler)
library(stringr)
## Read all articles collected so far
setwd("../Data/NYTimes")
Articles <- read.csv("NYTimes_Articles_Total")
Articles <- subset(Articles, select = -c(X))
setwd("../../code")
#Rcrawler(Website = "https://www.nytimes.com/aponline/2018/03/20/world/europe/ap-eu-britain-facebook-cambridge-analytica-the-latest.html", no_cores = 4, no_conn = 4, ExtractXpathPat = c("//h1","//article"), PatternsNames = c("Title","Content"))
## Using rcrawler to extract the article from URL (present in "Articles")
## try is used for error handling
## A total word count is done on the article to elimninate duplicates
try(for (i in c(1:nrow(Articles))){
if (i==1){
Data<-ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content"))
content <-data.frame(Data)
content$Content <- str_replace_all(content$Content, "[^[:alnum:]]", " ")  ## removes special characters
wordcount <- length(unlist(strsplit(toString(content$Content[i])," "))) ## gets a count of the number of words in the article
content <- cbind(content,wordcount)
}
if (i>1){
try(Data <- ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
temp <- data.frame(Data)
temp$Content <- str_replace_all(temp$Content, "[^[:alnum:]]", " ") ##removes special characters
wordcount <- length(unlist(strsplit(toString(temp$Content)," "))) ## gets a count of the number of words in the article
temp <- cbind(temp,wordcount)
content <- rbind(content,temp)
}
print(i)
})
## Eliminating duplicates caused by error handling
content = content[!duplicated(content$wordcount),]
## Saving the article extracted
setwd("../Data")
write.table(content$Content,"articlesTotal.txt",col.names = F,row.names = F)
setwd("../code")
library('RCurl')
library('XML')
library(Rcrawler)
library(stringr)
## Read all articles collected so far
setwd("../Data/NYTimes")
Articles <- read.csv("NYTimes_Articles_Total")
Articles <- subset(Articles, select = -c(X))
setwd("../../code")
#Rcrawler(Website = "https://www.nytimes.com/aponline/2018/03/20/world/europe/ap-eu-britain-facebook-cambridge-analytica-the-latest.html", no_cores = 4, no_conn = 4, ExtractXpathPat = c("//h1","//article"), PatternsNames = c("Title","Content"))
## Using rcrawler to extract the article from URL (present in "Articles")
## try is used for error handling
## A total word count is done on the article to elimninate duplicates
i=2
try(Data <- ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
i=35
try(Data <- ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
i=46
try(Data <- ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
i=2
## Saving the article extracted
setwd("../Data")
a<-read.table("articlesTotal.txt"")
sdf
sdf
sdf
]
eread.,afdkjsna
dslakfnlkdsaf
content = content[!duplicated(content$wordcount),]
end
;
""
a <- read.table("articlesTotal.txt"")
""
a <- read.table("articlesTotal.txt)
''
""
""
a <- read.table("articlesTotal.txt")
setwd("~/Documents/GitHub/Data-Integration-Big-Data-Analysis-And-Visualization/Part2/code")
library('RCurl')
library('XML')
library(Rcrawler)
library(stringr)
## Read all articles collected so far
setwd("../Data/NYTimes")
Articles <- read.csv("NYTimes_Articles_Total")
Articles <- subset(Articles, select = -c(X))
setwd("../../code")
for (i in c(1:nrow(Articles))){
if (i==1){
try(Data<-ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
content <-data.frame(Data)
content$Content <- str_replace_all(content$Content, "[^[:alnum:]]", " ")  ## removes special characters
wordcount <- length(unlist(strsplit(toString(content$Content[i])," "))) ## gets a count of the number of words in the article
content <- cbind(content,wordcount)
}
if (i>1){
try(Data <- ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
temp <- data.frame(Data)
temp$Content <- str_replace_all(temp$Content, "[^[:alnum:]]", " ") ##removes special characters
wordcount <- length(unlist(strsplit(toString(temp$Content)," "))) ## gets a count of the number of words in the article
temp <- cbind(temp,wordcount)
content <- rbind(content,temp)
}
print(i)
}
i=2
try(Data<-ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
i=3
try(Data<-ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
i=4
try(Data<-ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
for (i in c(1:nrow(Articles))){
if (i>1){
try(Data <- ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
temp <- data.frame(Data)
temp$Content <- str_replace_all(temp$Content, "[^[:alnum:]]", " ") ##removes special characters
wordcount <- length(unlist(strsplit(toString(temp$Content)," "))) ## gets a count of the number of words in the article
temp <- cbind(temp,wordcount)
content <- rbind(content,temp)
}
print(i)
}
for (i in c(1:nrow(Articles))){
if (i>1){
try(Data <- ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
#temp <- data.frame(Data)
#temp$Content <- str_replace_all(temp$Content, "[^[:alnum:]]", " ") ##removes special characters
#wordcount <- length(unlist(strsplit(toString(temp$Content)," "))) ## gets a count of the number of words in the article
#temp <- cbind(temp,wordcount)
#content <- rbind(content,temp)
}
print(i)
}
setwd("~/Documents/GitHub/Data-Integration-Big-Data-Analysis-And-Visualization/Part2/code")
library('RCurl')
library('XML')
library(Rcrawler)
library(stringr)
## Read all articles collected so far
setwd("../Data/NYTimes")
Articles <- read.csv("NYTimes_Articles_Total")
Articles <- subset(Articles, select = -c(X))
setwd("../../code")
#Rcrawler(Website = "https://www.nytimes.com/aponline/2018/03/20/world/europe/ap-eu-britain-facebook-cambridge-analytica-the-latest.html", no_cores = 4, no_conn = 4, ExtractXpathPat = c("//h1","//article"), PatternsNames = c("Title","Content"))
## Using rcrawler to extract the article from URL (present in "Articles")
## try is used for error handling
## A total word count is done on the article to elimninate duplicates
for (i in c(1:nrow(Articles))){
if (i==1){
try(Data<-ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
content <-data.frame(Data)
content$Content <- str_replace_all(content$Content, "[^[:alnum:]]", " ")  ## removes special characters
wordcount <- length(unlist(strsplit(toString(content$Content[i])," "))) ## gets a count of the number of words in the article
content <- cbind(content,wordcount)
}
if (i>1){
try(Data <- ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
temp <- data.frame(Data)
temp$Content <- str_replace_all(temp$Content, "[^[:alnum:]]", " ") ##removes special characters
wordcount <- length(unlist(strsplit(toString(temp$Content)," "))) ## gets a count of the number of words in the article
temp <- cbind(temp,wordcount)
content <- rbind(content,temp)
}
print(i)
}
## Eliminating duplicates caused by error handling
content = content[!duplicated(content$wordcount),]
## Saving the article extracted
setwd("../Data")
write.table(content$Content,"articlesTotal.txt",col.names = F,row.names = F)
setwd("../code")
## Saving the article extracted
setwd("../Data")
a<-read.table("articlesTotal.txt")
View(a)
a<-read.table("articlesOneDay.txt")
a<-read.table("tweetsOneDay.txt")
a<-read.table("tweetsTotal.txt")
library(rtimes)
Start_Date <- "20180310"
End_Data <- "20180405"
## Setting up Authentication
Sys.setenv(NYTIMES_AS_KEY = "fa567ce571174336957fc6786b4dc91e")
## Collecting of articles
DF <- as_search(q = "cambridge", "analytica", begin_date = Start_Date, end_date = End_Data, all_results = TRUE)
## Pre- processing ( removing duplicates)
Data <- DF$data
Data <- subset(Data, select = -c(multimedia, keywords, byline.person))
Data = Data[!duplicated(Data$`_id`),]
Data = Data[!duplicated(Data$snippet),]
Data = Data[!duplicated(Data$word_count),]
## Saving the articles collected for the day
setwd("../Data/NYTimes")
Name=paste("NYTimes","- Collected" ,nrow(Data)," articles on :",Sys.time())
write.csv(Data, Name)
setwd("../../code")
## Read all articles collected so far
setwd("../Data/NYTimes")
Articles <- read.csv("NYTimes_Articles_Total")
Articles <- subset(Articles, select = -c(X))
names(Articles)[names(Articles) == 'X_id'] <- '_id'
## Merge collected articles with existing articles and saving a consolidated csv file
Articles <- rbind(Articles,Data)
Articles = Articles[!duplicated(Articles$`_id`),]
Articles = Articles[!duplicated(Articles$snippet),]
Articles = Articles[!duplicated(Articles$word_count),]
write.csv(Articles, "NYTimes_Articles_Total")
setwd("../../code")
Articles <- read.csv("NYTimes_Articles_Total")
## Read all articles collected so far
setwd("../Data/NYTimes")
Articles <- read.csv("NYTimes_Articles_Total")
setwd("~/Documents/GitHub/Data-Integration-Big-Data-Analysis-And-Visualization/Part2/code")
library('RCurl')
library('XML')
library(Rcrawler)
library(stringr)
## Read all articles collected so far
setwd("../Data/NYTimes")
Articles <- read.csv("NYTimes_Articles_Total")
Articles <- subset(Articles, select = -c(X))
setwd("../../code")
for (i in c(1:nrow(Articles))){
if (i==1){
try(Data<-ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
content <-data.frame(Data)
content$Content <- str_replace_all(content$Content, "[^[:alnum:]]", " ")  ## removes special characters
wordcount <- length(unlist(strsplit(toString(content$Content[i])," "))) ## gets a count of the number of words in the article
content <- cbind(content,wordcount)
}
if (i>1){
try(Data <- ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
temp <- data.frame(Data)
temp$Content <- str_replace_all(temp$Content, "[^[:alnum:]]", " ") ##removes special characters
wordcount <- length(unlist(strsplit(toString(temp$Content)," "))) ## gets a count of the number of words in the article
temp <- cbind(temp,wordcount)
content <- rbind(content,temp)
}
print(i)
}
i=10
try(Data<-ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
content <-data.frame(Data)
content$Content <- str_replace_all(content$Content, "[^[:alnum:]]", " ")  ## removes special characters
wordcount <- length(unlist(strsplit(toString(content$Content[i])," "))) ## gets a count of the number of words in the article
content <- cbind(content,wordcount)
for (i in c(1:nrow(Articles))){
if (i==1){
try(Data<-ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
content <-data.frame(Data)
content$Content <- str_replace_all(content$Content, "[^[:alnum:]]", " ")  ## removes special characters
wordcount <- length(unlist(strsplit(toString(content$Content[i])," "))) ## gets a count of the number of words in the article
content <- cbind(content,wordcount)
}
if (i>1){
try(Data <- ContentScraper(Url = toString(Articles$web_url[i]), XpathPatterns =c("//h1","//article"), PatternsName = c("Title","Content")))
temp <- data.frame(Data)
temp$Content <- str_replace_all(temp$Content, "[^[:alnum:]]", " ") ##removes special characters
wordcount <- length(unlist(strsplit(toString(temp$Content)," "))) ## gets a count of the number of words in the article
temp <- cbind(temp,wordcount)
content <- rbind(content,temp)
}
print(i)
}
## Eliminating duplicates caused by error handling
content = content[!duplicated(content$wordcount),]
## Saving the article extracted
setwd("../Data")
write.table(content$Content,"articlesTotal.txt",col.names = F,row.names = F)
setwd("../code")
