View(tweets)
View(tweets)
View(tweets)
View(tweets)
View(tweets)
search.string <- "#flu"
no.of.tweets <- 100
tweets <- searchTwitter(search.string, n=no.of.tweets, lang="en")
tweets
View(tweets)
View(tweets)
rm(list =ls())
## loading libraries
library(twitteR)
library(ggplot2)
library(ggmap)
## Setup oauth
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A", "owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m", "340449785-0AWt3nkBVvLlX7hbUFLl0fEqIKs47qUU7V5UnFWH", "qnaD0Pyp9jUXfwVb82RlSKikuvVi2MAWxp1J0mD1Fle4d")
## Searching for tweets ##
search.string <- "flu"
no.of.tweets <- 15000
tweets <- searchTwitter(search.string, n=no.of.tweets, lang="en")
rm(list =ls())
## loading libraries
library(twitteR)
library(ggplot2)
library(ggmap)
## Setup oauth
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A", "owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m", "340449785-0AWt3nkBVvLlX7hbUFLl0fEqIKs47qUU7V5UnFWH", "qnaD0Pyp9jUXfwVb82RlSKikuvVi2MAWxp1J0mD1Fle4d")
## Searching for tweets ##
search.string <- "flu"
no.of.tweets <- 5000
tweets <- searchTwitter(search.string, n=no.of.tweets, lang="en")
search.string <- "flu"
no.of.tweets <- 5000
tweets <- searchTwitter(search.string, n=no.of.tweets, lang="en")
Name=Sys.Date()+Sys.time()
Name=paste(Sys.Date(),Sys.time(),sep = ";")
Name=Sys.time()
Name=paste("Data Collected on",Sys.time(),sep = ";")
Name=paste("Data Collected on ",Sys.time(),)
Name=paste("Data Collected on ",Sys.time())
rm(list =ls())
## loading libraries
library(twitteR)
library(ggplot2)
library(ggmap)
## Setup oauth
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A", "owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m", "340449785-0AWt3nkBVvLlX7hbUFLl0fEqIKs47qUU7V5UnFWH", "qnaD0Pyp9jUXfwVb82RlSKikuvVi2MAWxp1J0mD1Fle4d")
## Searching for tweets ##
search.string <- "flu"
no.of.tweets <- 100
tweets <- searchTwitter(search.string, n=no.of.tweets, lang="en")
## Conversion of searched tweets to Data frame
tweets <- twListToDF(tweets)
## Saving collected data to a csv file
Name=paste("Data Collected on ",Sys.time())
write.csv(tweets, file = Name)
install.packages("choroplethr")
data$ACTIVITY.LEVEL=as.numeric(data$ACTIVITY.LEVEL)
library(plotly)
?Sys.timezone
a <- ?Sys.timezone
Sys.getenv("TZ")
Sys.setenv("TZ")
as.POSIXct(t, tz=getOption("tz"))
install.packages("plotly")
Sys.setenv
Sys.setenv(TZ = "America/New_York")
install.packages("plotly")
library(rtimes)
Start_Date <- "20180310"
End_Data <- "20180406"
## Setting up Authentication
Sys.setenv(NYTIMES_AS_KEY = "fa567ce571174336957fc6786b4dc91e")
## Collecting of articles
DF <- as_search(q = "delete", "facebook", begin_date = Start_Date, end_date = End_Data, all_results = TRUE)
## Pre- processing ( removing duplicates)
Data <- DF$data
Data <- subset(Data, select = -c(multimedia, keywords, byline.person))
Data = Data[!duplicated(Data$`_id`),]
Data = Data[!duplicated(Data$snippet),]
Data = Data[!duplicated(Data$word_count),]
setwd("~/Documents/GitHub/Data-Integration-Big-Data-Analysis-And-Visualization/Part2/code")
## Saving the articles collected for the day
setwd("../Data/NYTimes")
Name=paste("NYTimes","- Collected" ,nrow(Data)," articles on :",Sys.time())
write.csv(Data, Name)
setwd("../../code")
## Read all articles collected so far
setwd("../Data/NYTimes")
Articles <- read.csv("NYTimes_Articles_Total")
Articles <- subset(Articles, select = -c(X))
names(Articles)[names(Articles) == 'X_id'] <- '_id'
## Merge collected articles with existing articles and saving a consolidated csv file
Articles <- rbind(Articles,Data)
Articles = Articles[!duplicated(Articles$`_id`),]
Articles = Articles[!duplicated(Articles$snippet),]
Articles = Articles[!duplicated(Articles$word_count),]
write.csv(Articles, "NYTimes_Articles_Total")
setwd("../../code")
library(rtimes)
Start_Date <- "20180310"
End_Data <- "20180406"
## Setting up Authentication
Sys.setenv(NYTIMES_AS_KEY = "fa567ce571174336957fc6786b4dc91e")
## Collecting of articles
DF <- as_search(q = "facebook", "scandal", begin_date = Start_Date, end_date = End_Data, all_results = TRUE)
library(rtimes)
Start_Date <- "20180310"
End_Data <- "20180406"
## Setting up Authentication
Sys.setenv(NYTIMES_AS_KEY = "fa567ce571174336957fc6786b4dc91e")
## Collecting of articles
DF <- as_search(q = "facebook", "scandal", begin_date = Start_Date, end_date = End_Data, all_results = TRUE)
## Pre- processing ( removing duplicates)
Data <- DF$data
Data <- subset(Data, select = -c(multimedia, keywords, byline.person))
Data = Data[!duplicated(Data$`_id`),]
Data = Data[!duplicated(Data$snippet),]
Data = Data[!duplicated(Data$word_count),]
## Saving the articles collected for the day
setwd("../Data/NYTimes")
Name=paste("NYTimes","- Collected" ,nrow(Data)," articles on :",Sys.time())
write.csv(Data, Name)
setwd("../../code")
## Read all articles collected so far
setwd("../Data/NYTimes")
Articles <- read.csv("NYTimes_Articles_Total")
Articles <- subset(Articles, select = -c(X))
names(Articles)[names(Articles) == 'X_id'] <- '_id'
## Merge collected articles with existing articles and saving a consolidated csv file
Articles <- rbind(Articles,Data)
Articles = Articles[!duplicated(Articles$`_id`),]
Articles = Articles[!duplicated(Articles$snippet),]
Articles = Articles[!duplicated(Articles$word_count),]
write.csv(Articles, "NYTimes_Articles_Total")
setwd("../../code")
library(rtimes)
Start_Date <- "20180305"
End_Data <- "20180406"
## Setting up Authentication
Sys.setenv(NYTIMES_AS_KEY = "fa567ce571174336957fc6786b4dc91e")
## Collecting of articles
DF <- as_search(q = "cambridge", "analytica", begin_date = Start_Date, end_date = End_Data, all_results = TRUE)
## Pre- processing ( removing duplicates)
Data <- DF$data
Data <- subset(Data, select = -c(multimedia, keywords, byline.person))
Data = Data[!duplicated(Data$`_id`),]
Data = Data[!duplicated(Data$snippet),]
Data = Data[!duplicated(Data$word_count),]
## Saving the articles collected for the day
setwd("../Data/NYTimes")
Name=paste("NYTimes","- Collected" ,nrow(Data)," articles on :",Sys.time())
write.csv(Data, Name)
setwd("../../code")
## Read all articles collected so far
setwd("../Data/NYTimes")
Articles <- read.csv("NYTimes_Articles_Total")
Articles <- subset(Articles, select = -c(X))
names(Articles)[names(Articles) == 'X_id'] <- '_id'
## Merge collected articles with existing articles and saving a consolidated csv file
Articles <- rbind(Articles,Data)
Articles = Articles[!duplicated(Articles$`_id`),]
Articles = Articles[!duplicated(Articles$snippet),]
Articles = Articles[!duplicated(Articles$word_count),]
write.csv(Articles, "NYTimes_Articles_Total")
setwd("../../code")
library(rtimes)
Start_Date <- "20180305"
End_Data <- "20180406"
## Setting up Authentication
Sys.setenv(NYTIMES_AS_KEY = "fa567ce571174336957fc6786b4dc91e")
## Collecting of articles
DF <- as_search(q = "cambridge", "analytica", begin_date = Start_Date, end_date = End_Data, all_results = TRUE)
## Pre- processing ( removing duplicates)
Data <- DF$data
Data <- subset(Data, select = -c(multimedia, keywords, byline.person))
Data = Data[!duplicated(Data$`_id`),]
Data = Data[!duplicated(Data$snippet),]
Data = Data[!duplicated(Data$word_count),]
## Saving the articles collected for the day
setwd("../Data/NYTimes")
Name=paste("NYTimes","- Collected" ,nrow(Data)," articles on :",Sys.time())
write.csv(Data, Name)
setwd("../../code")
## Read all articles collected so far
setwd("../Data/NYTimes")
Articles <- read.csv("NYTimes_Articles_Total")
Articles <- subset(Articles, select = -c(X))
names(Articles)[names(Articles) == 'X_id'] <- '_id'
## Merge collected articles with existing articles and saving a consolidated csv file
Articles <- rbind(Articles,Data)
Articles = Articles[!duplicated(Articles$`_id`),]
Articles = Articles[!duplicated(Articles$snippet),]
Articles = Articles[!duplicated(Articles$word_count),]
write.csv(Articles, "NYTimes_Articles_Total")
setwd("../../code")
library(rtimes)
Start_Date <- "20180301"
End_Data <- "20180406"
## Setting up Authentication
Sys.setenv(NYTIMES_AS_KEY = "fa567ce571174336957fc6786b4dc91e")
## Collecting of articles
DF <- as_search(q = "facebook", "scandal", begin_date = Start_Date, end_date = End_Data, all_results = TRUE)
## Pre- processing ( removing duplicates)
Data <- DF$data
Data <- subset(Data, select = -c(multimedia, keywords, byline.person))
Data = Data[!duplicated(Data$`_id`),]
Data = Data[!duplicated(Data$snippet),]
Data = Data[!duplicated(Data$word_count),]
## Saving the articles collected for the day
setwd("../Data/NYTimes")
Name=paste("NYTimes","- Collected" ,nrow(Data)," articles on :",Sys.time())
write.csv(Data, Name)
setwd("../../code")
## Read all articles collected so far
setwd("../Data/NYTimes")
Articles <- read.csv("NYTimes_Articles_Total")
Articles <- subset(Articles, select = -c(X))
names(Articles)[names(Articles) == 'X_id'] <- '_id'
## Merge collected articles with existing articles and saving a consolidated csv file
Articles <- rbind(Articles,Data)
Articles = Articles[!duplicated(Articles$`_id`),]
Articles = Articles[!duplicated(Articles$snippet),]
Articles = Articles[!duplicated(Articles$word_count),]
write.csv(Articles, "NYTimes_Articles_Total")
setwd("../../code")
library(rtimes)
Start_Date <- "20180301"
End_Data <- "20180406"
## Setting up Authentication
Sys.setenv(NYTIMES_AS_KEY = "fa567ce571174336957fc6786b4dc91e")
## Collecting of articles
DF <- as_search(q = "delete", "facebook", begin_date = Start_Date, end_date = End_Data, all_results = TRUE)
## Pre- processing ( removing duplicates)
Data <- DF$data
Data <- subset(Data, select = -c(multimedia, keywords, byline.person))
Data = Data[!duplicated(Data$`_id`),]
Data = Data[!duplicated(Data$snippet),]
Data = Data[!duplicated(Data$word_count),]
## Saving the articles collected for the day
setwd("../Data/NYTimes")
Name=paste("NYTimes","- Collected" ,nrow(Data)," articles on :",Sys.time())
write.csv(Data, Name)
setwd("../../code")
## Read all articles collected so far
setwd("../Data/NYTimes")
Articles <- read.csv("NYTimes_Articles_Total")
Articles <- subset(Articles, select = -c(X))
names(Articles)[names(Articles) == 'X_id'] <- '_id'
## Merge collected articles with existing articles and saving a consolidated csv file
Articles <- rbind(Articles,Data)
Articles = Articles[!duplicated(Articles$`_id`),]
Articles = Articles[!duplicated(Articles$snippet),]
Articles = Articles[!duplicated(Articles$word_count),]
write.csv(Articles, "NYTimes_Articles_Total")
setwd("../../code")
library(rtimes)
Start_Date <- "20180301"
End_Data <- "20180406"
## Setting up Authentication
Sys.setenv(NYTIMES_AS_KEY = "fa567ce571174336957fc6786b4dc91e")
## Collecting of articles
DF <- as_search(q = "mark", "zuckerberg", begin_date = Start_Date, end_date = End_Data, all_results = TRUE)
library(rtimes)
Start_Date <- "20180301"
End_Data <- "20180406"
## Setting up Authentication
Sys.setenv(NYTIMES_AS_KEY = "fa567ce571174336957fc6786b4dc91e")
## Collecting of articles
DF <- as_search(q = "facebook", "data", begin_date = Start_Date, end_date = End_Data, all_results = TRUE)
## Pre- processing ( removing duplicates)
Data <- DF$data
Data <- subset(Data, select = -c(multimedia, keywords, byline.person))
Data = Data[!duplicated(Data$`_id`),]
Data = Data[!duplicated(Data$snippet),]
Data = Data[!duplicated(Data$word_count),]
View(Data)
## Saving the articles collected for the day
setwd("../Data/NYTimes")
Name=paste("NYTimes","- Collected" ,nrow(Data)," articles on :",Sys.time())
write.csv(Data, Name)
setwd("../../code")
## Read all articles collected so far
setwd("../Data/NYTimes")
Articles <- read.csv("NYTimes_Articles_Total")
Articles <- subset(Articles, select = -c(X))
names(Articles)[names(Articles) == 'X_id'] <- '_id'
## Merge collected articles with existing articles and saving a consolidated csv file
Articles <- rbind(Articles,Data)
Articles = Articles[!duplicated(Articles$`_id`),]
Articles = Articles[!duplicated(Articles$snippet),]
Articles = Articles[!duplicated(Articles$word_count),]
write.csv(Articles, "NYTimes_Articles_Total")
setwd("../../code")
library('RCurl')
library('XML')
library(Rcrawler)
library(stringr)
## Read all articles collected so far
setwd("../Data/NYTimes")
Articles <- read.csv("NYTimes_Articles_Total")
Articles <- subset(Articles, select = -c(X))
setwd("../../code")
setwd("~/Documents/GitHub/Data-Integration-Big-Data-Analysis-And-Visualization/Part2/code")
for (i in c(1:nrow(Articles))){
if (i==1){
try(Data<-ContentScraper(Url = toString(Articles$web_url[i]),
XpathPatterns =c("//*/p[@class='story-body-text story-content']"),
#ExcludeXpathPat = c("//*/header[@id='story-header']",
#                    "//*/div[@class='story-interrupter']",
#                   "//*/div[@class='story-interrupter']",
#                  "//*/footer[@class='story-footer story-content']",
#                 "//*/section[@id='related-combined-coverage']",
#                "//*/div[@class='reader-satisfaction-survey prompt feedback-prompt story-content hidden']",
#               "//*/div[@id='story-meta-footer']"),
PatternsName = c("Content"),
ManyPerPattern = TRUE))
content <-data.frame(Data)
content$Content <- str_replace_all(content$Content, "[^[:alnum:]]", " ")  ## removes special characters
#wordcount <- length(unlist(strsplit(toString(content$Content[i])," "))) ## gets a count of the number of words in the article
#content <- cbind(content,wordcount)
content<-as.vector(t(as.matrix(content)))
content<-data.frame(paste(content, collapse = ''))
colnames(content) <- "content"
wordcount <- length(unlist(strsplit(toString(content$content)," "))) ## gets a count of the number of words in the article
content <- cbind(content,wordcount)
}
if (i>1){
try(Data<-ContentScraper(Url = toString(Articles$web_url[i]),
XpathPatterns =c("//*/p[@class='story-body-text story-content']"),
#ExcludeXpathPat = c("//*/header[@id='story-header']",
#                    "//*/div[@class='story-interrupter']",
#                   "//*/div[@class='story-interrupter']",
#                  "//*/footer[@class='story-footer story-content']",
#                 "//*/section[@id='related-combined-coverage']",
#                "//*/div[@class='reader-satisfaction-survey prompt feedback-prompt story-content hidden']",
#               "//*/div[@id='story-meta-footer']"),
PatternsName = c("Content"),
ManyPerPattern = TRUE))
temp <- data.frame(Data)
temp$Content <- str_replace_all(temp$Content, "[^[:alnum:]]", " ") ##removes special characters
temp<-as.vector(t(as.matrix(temp)))
temp<-data.frame(paste(temp, collapse = ''))
colnames(temp) <- "content"
wordcount <- length(unlist(strsplit(toString(temp$content)," "))) ## gets a count of the number of words in the article
temp <- cbind(temp,wordcount)
content <- rbind(content,temp)
}
print(i)
}
i=7
try(Data<-ContentScraper(Url = toString(Articles$web_url[i]),
XpathPatterns =c("//*/p[@class='story-body-text story-content']"),
#ExcludeXpathPat = c("//*/header[@id='story-header']",
#                    "//*/div[@class='story-interrupter']",
#                   "//*/div[@class='story-interrupter']",
#                  "//*/footer[@class='story-footer story-content']",
#                 "//*/section[@id='related-combined-coverage']",
#                "//*/div[@class='reader-satisfaction-survey prompt feedback-prompt story-content hidden']",
#               "//*/div[@id='story-meta-footer']"),
PatternsName = c("Content"),
ManyPerPattern = TRUE))
i=10
try(Data<-ContentScraper(Url = toString(Articles$web_url[i]),
XpathPatterns =c("//*/p[@class='story-body-text story-content']"),
#ExcludeXpathPat = c("//*/header[@id='story-header']",
#                    "//*/div[@class='story-interrupter']",
#                   "//*/div[@class='story-interrupter']",
#                  "//*/footer[@class='story-footer story-content']",
#                 "//*/section[@id='related-combined-coverage']",
#                "//*/div[@class='reader-satisfaction-survey prompt feedback-prompt story-content hidden']",
#               "//*/div[@id='story-meta-footer']"),
PatternsName = c("Content"),
ManyPerPattern = TRUE))
content <-data.frame(Data)
content$Content <- str_replace_all(content$Content, "[^[:alnum:]]", " ")  ## removes special characters
#wordcount <- length(unlist(strsplit(toString(content$Content[i])," "))) ## gets a count of the number of words in the article
#content <- cbind(content,wordcount)
content<-as.vector(t(as.matrix(content)))
content<-data.frame(paste(content, collapse = ''))
colnames(content) <- "content"
wordcount <- length(unlist(strsplit(toString(content$content)," "))) ## gets a count of the number of words in the article
content <- cbind(content,wordcount)
for (i in c(1:nrow(Articles))){
if (i==1){
try(Data<-ContentScraper(Url = toString(Articles$web_url[i]),
XpathPatterns =c("//*/p[@class='story-body-text story-content']"),
#ExcludeXpathPat = c("//*/header[@id='story-header']",
#                    "//*/div[@class='story-interrupter']",
#                   "//*/div[@class='story-interrupter']",
#                  "//*/footer[@class='story-footer story-content']",
#                 "//*/section[@id='related-combined-coverage']",
#                "//*/div[@class='reader-satisfaction-survey prompt feedback-prompt story-content hidden']",
#               "//*/div[@id='story-meta-footer']"),
PatternsName = c("Content"),
ManyPerPattern = TRUE))
content <-data.frame(Data)
content$Content <- str_replace_all(content$Content, "[^[:alnum:]]", " ")  ## removes special characters
#wordcount <- length(unlist(strsplit(toString(content$Content[i])," "))) ## gets a count of the number of words in the article
#content <- cbind(content,wordcount)
content<-as.vector(t(as.matrix(content)))
content<-data.frame(paste(content, collapse = ''))
colnames(content) <- "content"
wordcount <- length(unlist(strsplit(toString(content$content)," "))) ## gets a count of the number of words in the article
content <- cbind(content,wordcount)
}
if (i>1){
try(Data<-ContentScraper(Url = toString(Articles$web_url[i]),
XpathPatterns =c("//*/p[@class='story-body-text story-content']"),
#ExcludeXpathPat = c("//*/header[@id='story-header']",
#                    "//*/div[@class='story-interrupter']",
#                   "//*/div[@class='story-interrupter']",
#                  "//*/footer[@class='story-footer story-content']",
#                 "//*/section[@id='related-combined-coverage']",
#                "//*/div[@class='reader-satisfaction-survey prompt feedback-prompt story-content hidden']",
#               "//*/div[@id='story-meta-footer']"),
PatternsName = c("Content"),
ManyPerPattern = TRUE))
temp <- data.frame(Data)
temp$Content <- str_replace_all(temp$Content, "[^[:alnum:]]", " ") ##removes special characters
temp<-as.vector(t(as.matrix(temp)))
temp<-data.frame(paste(temp, collapse = ''))
colnames(temp) <- "content"
wordcount <- length(unlist(strsplit(toString(temp$content)," "))) ## gets a count of the number of words in the article
temp <- cbind(temp,wordcount)
content <- rbind(content,temp)
}
print(i)
}
## Eliminating duplicates caused by error handling
content = content[!duplicated(content$wordcount),]
## Saving the article extracted
setwd("../Data")
write.table(content$content,"articlesTotal.txt",col.names = F,row.names = F)
setwd("../code")
## Saving the article extracted
setwd("../Data")
write.table(content$content,"articlesTotal.txt",col.names = F,row.names = F, quote = F)
setwd("../code")
## Saving the article extracted
setwd("../Data")
write.table(content$content,"articlesTotal.txt",col.names = F,row.names = F)
setwd("../code")
## Saving the article extracted
setwd("../Data")
write.table(content$content,"articlesTotal.txt",col.names = F,row.names = F, quote = F)
setwd("../code")
setwd("~/Documents/GitHub/Data-Integration-Big-Data-Analysis-And-Visualization/Part2/code")
##rm(list =ls())
#### Code to collect tweets##
## All tweets collected are filter and appended to csv files in data folder automatically
library(twitteR)
library(ggplot2)
library(ggmap)
library(data.table)
library(stringr)
Start_date <- "2018-03-10"
End_date <- "2018-04-06"
## Setup oauth
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A",
"owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m",
"340449785-0AWt3nkBVvLlX7hbUFLl0fEqIKs47qUU7V5UnFWH",
"qnaD0Pyp9jUXfwVb82RlSKikuvVi2MAWxp1J0mD1Fle4d")
############## Collection of Tweets ###################
## Searching for tweets ##
search.string <- c("facebook","data")
no.of.tweets <- 2500
tweets <- searchTwitter(search.string, n=no.of.tweets, lang="en", since= Start_date , until = End_date)
## Conversion of searched tweets to Data frame
tweets <- twListToDF(tweets)
## Saving collected data to a csv file - only the tweets collection this session
setwd("../Data/Twitter")
Name=paste("Twitter -", no.of.tweets," Tweets Collected on :",Sys.time())
write.csv(tweets, file = Name)
setwd("../../code")
# Reading all tweets collected so far
Tweets_Collected=read.csv("../data/Twitter/Tweets_Collected")
Tweets_Collected<- subset(Tweets_Collected, select = -c(X)) #removing column named X
temp <- Tweets_Collected
# Creating a consolided data frame of all the tweets collected so far
Tweets_Collected <- rbind(Tweets_Collected,tweets)
Tweets_Collected = Tweets_Collected[!duplicated(Tweets_Collected$id),]
# Saving all consolidated Tweets Collected to a csv file - before preprocessing
setwd("../Data/Twitter")
write.csv(Tweets_Collected, file = "Tweets_Collected")
setwd("../../code")
## Remove non- ASCII characters, hastags (#xxxxx) used in tweet search, tags(@xxxxxx) and other special characters
Tweets_Collected_prepocessed <- data.frame(iconv(Tweets_Collected$text, "latin1", "ASCII", sub=""))
Tweets_Collected_prepocessed <- rm_url(Tweets_Collected_prepocessed$iconv.Tweets_Collected.text...latin1....ASCII...sub......)
Tweets_Collected_prepocessed <- sub("#\\w+ *", "", Tweets_Collected_prepocessed)
Tweets_Collected_prepocessed <- data.frame(sub("@\\w+ *", "", Tweets_Collected_prepocessed))
Tweets_Collected_prepocessed <- data.frame(gsub("RT", "", Tweets_Collected_prepocessed$sub.....w..........Tweets_Collected_prepocessed.))
library(qdapRegex)
## Remove non- ASCII characters, hastags (#xxxxx) used in tweet search, tags(@xxxxxx) and other special characters
Tweets_Collected_prepocessed <- data.frame(iconv(Tweets_Collected$text, "latin1", "ASCII", sub=""))
Tweets_Collected_prepocessed <- rm_url(Tweets_Collected_prepocessed$iconv.Tweets_Collected.text...latin1....ASCII...sub......)
Tweets_Collected_prepocessed <- sub("#\\w+ *", "", Tweets_Collected_prepocessed)
Tweets_Collected_prepocessed <- data.frame(sub("@\\w+ *", "", Tweets_Collected_prepocessed))
Tweets_Collected_prepocessed <- data.frame(gsub("RT", "", Tweets_Collected_prepocessed$sub.....w..........Tweets_Collected_prepocessed.))
Tweets_Collected_prepocessed <- data.frame(str_replace_all(Tweets_Collected_prepocessed$gsub..RT.......Tweets_Collected_prepocessed.sub.....w..........Tweets_Collected_prepocessed..,
"[^[:alnum:]]", " "))
colnames(Tweets_Collected_prepocessed) <- c("content")
## Saves all tweets collected to a csv file - After PreProcessing
setwd("../Data")
write.table(Tweets_Collected_prepocessed$content, file = "tweetsTotal.txt", sep="\t", col.names = F, row.names = F)
setwd("../code")
## Saves all tweets collected to a csv file - After PreProcessing
setwd("../Data")
write.table(Tweets_Collected_prepocessed$content, file = "tweetsTotal.txt", sep="\t", col.names = F, row.names = F, quote = F)
setwd("../code")
