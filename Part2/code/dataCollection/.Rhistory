q()
q()
q()
q()
defaults write org.R-project.R force.LANG en_US.UTF-8
q()
install.packages("ggplot")
install.packages("ggplot2")
jupyter notebook
q()
install.packages("sp")
install.packages('gglplot2', dependencies = TRUE)
install.packages('ggplot2', dependencies = TRUE)
warnings()
q()
remove.packages("ggplot2")
install.packages('ggplot2', dependencies = TRUE)
q()
install.packages("sp")
conda install clangxx_osx-64
conda install clangxx_osx-64
q()
install.packages("twitteR")
install.packages(c("devtools", "rjson", "bit64", "httr"))
q()
library(twitteR)
install.packages("twitteR")
q(0
q()
q()
install.packages("twitteR", dependencies = TRUE)
library(twitteR)
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A", "owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m")
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A", "owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m")
install.packages("httpuv")
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A", "owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m")
install.packages("httpuv", dependencies=TRUE)
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A", "owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m")
devtools::install_github("rstudio/httpuv")
install_github("rstudio/httpuv")
getTwitterOAuth(consumer_key, consumer_secret)
getTwitterOAuth(VxJ6qp5XL3VTclBzMBsD1Ez1A, owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m)
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A", "owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m")
remove.packages("httpuv")
install.packages("httpuv")
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A", "owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m")
cred <- OAuthFactory$new(consumerKey='VxJ6qp5XL3VTclBzMBsD1Ez1A',
consumerSecret='owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m',
requestURL='https://api.twitter.com/oauth/request_token',
accessURL='https://api.twitter.com/oauth/access_token',
authURL='https://api.twitter.com/oauth/authorize')
search.string <- "#flu"
no.of.tweets <- 100
tweets <- searchTwitter(search.string, n=no.of.tweets, cainfo="cacert.pem", lang="en")
tweets
search.string <- "#flu"
no.of.tweets <- 100
tweets <- searchTwitter(search.string, n=no.of.tweets,lang="en")
tweets
View(tweets)
View(tweets)
View(tweets)
search.string <- "#flu"
no.of.tweets <- 10000
tweets <- searchTwitter(search.string, n=no.of.tweets, geocode="40.7128,74.0060,150mi", lang="en")
tweets
View(tweets)
View(tweets)
View(tweets)
tweets
search.string <- "#flu"
no.of.tweets <- 10000
tweets <- searchTwitter(search.string, n=no.of.tweets, geocode="40.7128,74.0060,150mi", lang="en")
tweets
View(tweets)
View(tweets)
View(tweets)
View(tweets)
search.string <- "#flu"
no.of.tweets <- 100
tweets <- searchTwitter(search.string, n=no.of.tweets, lang="en")
tweets
View(tweets)
View(tweets)
View(tweets)
View(tweets)
View(tweets)
View(tweets)
search.string <- "#flu"
no.of.tweets <- 100
tweets <- searchTwitter(search.string, n=no.of.tweets, lang="en")
tweets
View(tweets)
View(tweets)
rm(list =ls())
## loading libraries
library(twitteR)
library(ggplot2)
library(ggmap)
## Setup oauth
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A", "owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m", "340449785-0AWt3nkBVvLlX7hbUFLl0fEqIKs47qUU7V5UnFWH", "qnaD0Pyp9jUXfwVb82RlSKikuvVi2MAWxp1J0mD1Fle4d")
## Searching for tweets ##
search.string <- "flu"
no.of.tweets <- 15000
tweets <- searchTwitter(search.string, n=no.of.tweets, lang="en")
rm(list =ls())
## loading libraries
library(twitteR)
library(ggplot2)
library(ggmap)
## Setup oauth
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A", "owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m", "340449785-0AWt3nkBVvLlX7hbUFLl0fEqIKs47qUU7V5UnFWH", "qnaD0Pyp9jUXfwVb82RlSKikuvVi2MAWxp1J0mD1Fle4d")
## Searching for tweets ##
search.string <- "flu"
no.of.tweets <- 5000
tweets <- searchTwitter(search.string, n=no.of.tweets, lang="en")
search.string <- "flu"
no.of.tweets <- 5000
tweets <- searchTwitter(search.string, n=no.of.tweets, lang="en")
Name=Sys.Date()+Sys.time()
Name=paste(Sys.Date(),Sys.time(),sep = ";")
Name=Sys.time()
Name=paste("Data Collected on",Sys.time(),sep = ";")
Name=paste("Data Collected on ",Sys.time(),)
Name=paste("Data Collected on ",Sys.time())
rm(list =ls())
## loading libraries
library(twitteR)
library(ggplot2)
library(ggmap)
## Setup oauth
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A", "owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m", "340449785-0AWt3nkBVvLlX7hbUFLl0fEqIKs47qUU7V5UnFWH", "qnaD0Pyp9jUXfwVb82RlSKikuvVi2MAWxp1J0mD1Fle4d")
## Searching for tweets ##
search.string <- "flu"
no.of.tweets <- 100
tweets <- searchTwitter(search.string, n=no.of.tweets, lang="en")
## Conversion of searched tweets to Data frame
tweets <- twListToDF(tweets)
## Saving collected data to a csv file
Name=paste("Data Collected on ",Sys.time())
write.csv(tweets, file = Name)
install.packages("choroplethr")
data$ACTIVITY.LEVEL=as.numeric(data$ACTIVITY.LEVEL)
library(plotly)
?Sys.timezone
a <- ?Sys.timezone
Sys.getenv("TZ")
Sys.setenv("TZ")
as.POSIXct(t, tz=getOption("tz"))
install.packages("plotly")
Sys.setenv
Sys.setenv(TZ = "America/New_York")
install.packages("plotly")
setwd("~/Documents/GitHub/Data-Integration-Big-Data-Analysis-And-Visualization/Part2/code/dataCollection")
# Reading all tweets collected so far
Tweets_Collected=read.csv("../data/Twitter/Tweets_Collected")
# Reading all tweets collected so far
Tweets_Collected=read.csv("../../data/Twitter/Tweets_Collected")
# Reading all tweets collected so far
Tweets_Collected=read.csv("../../Data/Twitter/Tweets_Collected")
# Reading all tweets collected so far
Tweets_Collected=read.csv("../../Data/Twitter/Tweets_Collected")
##rm(list =ls())
#### Code to collect tweets##
## All tweets collected are filter and appended to csv files in data folder automatically
library(twitteR)
library(ggplot2)
library(ggmap)
library(data.table)
library(stringr)
library(qdapRegex)
Start_date <- "2018-03-10"
End_date <- "2018-04-06"
## Setup oauth
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A",
"owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m",
"340449785-0AWt3nkBVvLlX7hbUFLl0fEqIKs47qUU7V5UnFWH",
"qnaD0Pyp9jUXfwVb82RlSKikuvVi2MAWxp1J0mD1Fle4d")
##rm(list =ls())
#### Code to collect tweets##
## All tweets collected are filter and appended to csv files in data folder automatically
library(twitteR)
library(ggplot2)
library(ggmap)
library(data.table)
library(stringr)
library(qdapRegex)
Start_date <- "2018-03-10"
End_date <- "2018-04-06"
## Setup oauth
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A",
"owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m",
"340449785-0AWt3nkBVvLlX7hbUFLl0fEqIKs47qUU7V5UnFWH",
"qnaD0Pyp9jUXfwVb82RlSKikuvVi2MAWxp1J0mD1Fle4d")
############## Collection of Tweets ###################
## Searching for tweets ##
search.string <- c("facebook","data")
no.of.tweets <- 250
tweets <- searchTwitter(search.string, n=no.of.tweets, lang="en", since= Start_date , until = End_date)
## Conversion of searched tweets to Data frame
tweets <- twListToDF(tweets)
## Saving collected data to a csv file - only the tweets collection this session
setwd("../../Data/Twitter")
Name=paste("Twitter -", no.of.tweets," Tweets Collected on :",Sys.time())
write.csv(tweets, file = Name)
setwd("../../code/dataCollection")
# Reading all tweets collected so far
Tweets_Collected=read.csv("../../Data/Twitter/Tweets_Collected")
Tweets_Collected<- subset(Tweets_Collected, select = -c(X)) #removing column named X
temp <- Tweets_Collected
# Creating a consolided data frame of all the tweets collected so far
Tweets_Collected <- rbind(Tweets_Collected,tweets)
Tweets_Collected = Tweets_Collected[!duplicated(Tweets_Collected$id),]
# Saving all consolidated Tweets Collected to a csv file - before preprocessing
setwd("../../Data/Twitter")
setwd("../../code/dataCollection")
setwd("~/Documents/GitHub/Data-Integration-Big-Data-Analysis-And-Visualization/Part2/code/dataCollection")
## Read all articles collected so far
setwd("../../Data/NYTimes")
setwd("../../code/dataCollection")
## Saving the article extracted
setwd("../../Data")
setwd("../code/dataCollection")
## Saving the articles collected for the day
setwd("../../Data/NYTimes")
setwd("../../code/dataCollection")
## Saving the articles collected for the day
setwd("../../Data/NYTimes")
setwd("../../code/dataCollection")
## Read all articles collected so far
setwd("../../Data/NYTimes")
setwd("../../code/dataCollection")
